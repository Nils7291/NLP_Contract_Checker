{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fff9637",
   "metadata": {},
   "source": [
    "# NLP Project Sectorlense Contract checker\n",
    "\n",
    "**Projectdescription**\n",
    "\n",
    "Reviewing software contracts is often a complex and error-prone task, particularly when\n",
    "assessing standardized requirements and identifying potential risks. Manual contract review\n",
    "can be time-consuming, leading to inconsistencies and oversight. To address this challenge,\n",
    "the project aims to develop an LLM-based contract checker that automates the review\n",
    "process. By leveraging predefined checklists and legal standards, the system will\n",
    "systematically analyze contracts, ensuring that required clauses are present while also\n",
    "detecting critical or unusual formulations. This will streamline contract evaluation and\n",
    "facilitate structured risk assessment, reducing both time and effort for legal professionals\n",
    "and businesses.\n",
    "\n",
    "The contract checker will incorporate three primary functionalities. A standard compliance\n",
    "check will verify whether contracts include the necessary clauses and if they adhere to\n",
    "established legal and business standards. Assessment based on standardized criteria will\n",
    "evaluate key contractual aspects to ensure completeness and compliance. Risk identificatalogue_rawion\n",
    "will highlight non-standard, ambiguous, or high-risk clauses, enabling users to assess their\n",
    "appropriateness compared to standard contract terms. Additionally, an optional risk\n",
    "detection feature could be introduced to flag further potential risks that may not be explicitly\n",
    "covered in the predefined checklist.\n",
    "\n",
    "The final deliverable will be a web application that enables users to upload contract\n",
    "documents and receive an automated structured review including insights on compliance\n",
    "and risk factors. This application will provide detailed feedback, highlight critical sections,\n",
    "and suggest improvements, making contract review more efficient and reliable.\n",
    "Development will build upon an existing prototype that includes both a frontend and basic\n",
    "functionality, allowing for enhancements in accuracy, usability, and scalability.\n",
    "\n",
    "**Meilensteine**:\n",
    "\n",
    "Milestone 1: Understanding existing prototype and defining key requirements (Week 1-2)\n",
    "\n",
    "Milestone 2: Developing/improving NLP-based contract analysis model (Week 3-6)\n",
    "\n",
    "Milestone 3: Integration into the web application (Week 7-8)\n",
    "\n",
    "Milestone 4: Testing and evaluation with real-world contracts (Week 9-10)\n",
    "\n",
    "Milestone 5: Final presentation and documentation (Week 11-12)\n",
    "\n",
    "**Data**\n",
    "\n",
    "Contract documents in various formats (PDF, DOCX, TXT). Predefined checklists and legal standards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1ac2874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/dave/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "#  SYSTEM & ENVIRONMENT\n",
    "# ==============================================================================\n",
    "import os\n",
    "import sys\n",
    "import ssl\n",
    "import certifi\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# SSL-Config (NLTK, Requests)\n",
    "ssl._create_default_https_context = lambda: ssl.create_default_context(cafile=certifi.where())\n",
    "\n",
    "# ==============================================================================\n",
    "#  DATA HANDLING\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==============================================================================\n",
    "#  TEXT PROCESSING & NLP\n",
    "# ==============================================================================\n",
    "import string\n",
    "import re\n",
    "from itertools import chain\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import en_core_web_sm\n",
    "\n",
    "from gensim.parsing.preprocessing import (\n",
    "    STOPWORDS,\n",
    "    strip_tags, strip_numeric, strip_punctuation,\n",
    "    strip_multiple_whitespaces, remove_stopwords,\n",
    "    strip_short, stem_text\n",
    ")\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# ==============================================================================\n",
    "#  FILE READING & SCRAPING\n",
    "# ==============================================================================\n",
    "import pdfplumber\n",
    "import docx\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ==============================================================================\n",
    "#  VISUALIZATION\n",
    "# ==============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# ==============================================================================\n",
    "#  MACHINE LEARNING / DEEP LEARNING\n",
    "# ==============================================================================\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, accuracy_score,\n",
    "    f1_score, recall_score, roc_curve, auc\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Transformers & Sentence Embeddings\n",
    "from transformers import (\n",
    "    BertTokenizer, BertModel,\n",
    "    AutoTokenizer, AutoModel, AutoConfig\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from sentence_transformers.models import Pooling\n",
    "import inspect\n",
    "\n",
    "# ==============================================================================\n",
    "#  OPENAI API AND JSON HANDLING\n",
    "# ==============================================================================\n",
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from key import OpenAiKey\n",
    "\n",
    "# ==============================================================================\n",
    "#  REPRODUCIBILITY\n",
    "# ==============================================================================\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "#  Custom Functions and Classes\n",
    "# ==============================================================================\n",
    "## scraping and reading files\n",
    "from functions.function_contract_read_in import (  \n",
    "    scrape_html_standard\n",
    "    , scrape_html_commonpaper\n",
    "    , scrape_html_fakturia\n",
    "    , scrape_html_mitratech\n",
    "    , scrape_contract_auto\n",
    "    , read_txt_file\n",
    ")\n",
    "## text processing\n",
    "from functions.functions_preprocessing import( \n",
    "    extract_paragraphs_and_sections\n",
    "    , extract_title_fixed\n",
    "    , clean_sections_and_paragraphs\n",
    ")\n",
    "## embeddings\n",
    "from functions.functions_embeddings import add_embed_text_column\n",
    "\n",
    "\n",
    "# Cosine Mapper\n",
    "from classes.class_model import CosineMapper\n",
    "# textlabeldataset\n",
    "from classes.Class_TextLabelDataset import TextLabelDataset\n",
    "# Predictor\n",
    "from classes.class_predictor import SectionTopicPredictor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83edca74",
   "metadata": {},
   "source": [
    "# 1. Read in Contracts\n",
    "\n",
    "The contract checker tool that is going to be created in this project needs to be tested and trained based on some real world example contracts. Therefore Sectorelense provided us with an excel sheet containing a list of various providers of Saas solutions and links to their websites where sample contracts are available.\n",
    "\n",
    "These contract documents appear in various formats. Some of them in HTML, some in PDF, some in DOCX and some in the format of JSON.\n",
    "\n",
    "To automate the collection of contracts our first approach was to try to build an automated scraping tool for each file format.\n",
    "\n",
    "## 1.1 Scraping HTML\n",
    "We Started by creating a scraping tool for HTML websites. After a short time we realised that this woulden´t be as easy as expected, since all the websites appear in different formats which leads to different scraping properties for every website.\n",
    "\n",
    "However we proceeded and tried to build a seperate scraping function for all the provided websites that seemed to be impactfull to us.\n",
    "\n",
    "The following code shows scraping functions for different kind of websites. In the end you can find a chooser function, that chooses which scraping functtion to use exactly based on the link provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245020b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # 1. Scraper für Standard-HTML-Verträge\n",
    "# def scrape_html_standard(url):\n",
    "#     try:\n",
    "#         headers = {\n",
    "#             \"User-Agent\": (\n",
    "#                 \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "#                 \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "#                 \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "#             )\n",
    "#         }\n",
    "#         response = requests.get(url, headers=headers)\n",
    "#         response.encoding = 'utf-8'\n",
    "#         response.raise_for_status()\n",
    "\n",
    "#         soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#         for tag in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\"]):\n",
    "#             tag.decompose()\n",
    "\n",
    "#         main_content = soup.find(\"div\", class_=\"single-content\") or soup\n",
    "#         raw_text = main_content.get_text(separator=\" \", strip=True)\n",
    "#         full_text = re.sub(r'\\s+', ' ', raw_text)\n",
    "\n",
    "#         start_patterns = [r\"§\\s?\\d+\", r\"1\\.\\s+[^\\n\\.]+\"]\n",
    "#         for pattern in start_patterns:\n",
    "#             match = re.search(pattern, full_text)\n",
    "#             if match:\n",
    "#                 full_text = full_text[match.start():]\n",
    "#                 break\n",
    "\n",
    "#         end_markers = [\n",
    "#             \"Die eingetragene Marke MOCO\", \"Stand 12/2024\", \"Ort, Datum\",\n",
    "#             \"Unterschrift\", \"Impressum\", \"©\", \"Nachtrag Australische spezifische Begriffe\"\n",
    "#         ]\n",
    "#         cutoff = int(len(full_text) * 0.7)\n",
    "#         positions = {m: full_text.find(m) for m in end_markers if full_text.find(m) > cutoff}\n",
    "#         if positions:\n",
    "#             full_text = full_text[:min(positions.values())]\n",
    "\n",
    "#         return full_text.strip()\n",
    "\n",
    "#     except Exception:\n",
    "#         return \"\"\n",
    "\n",
    "\n",
    "# # 2. Scraper für CommonPaper-Verträge\n",
    "# def scrape_html_commonpaper(url):\n",
    "#     try:\n",
    "#         response = requests.get(url)\n",
    "#         response.raise_for_status()\n",
    "\n",
    "#         soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#         content = soup.find(\"div\", class_=\"entry-content\")\n",
    "#         if not content:\n",
    "#             print(f\"⚠️ CommonPaper: Kein Hauptbereich gefunden – {url}\")\n",
    "#             return \"\"\n",
    "\n",
    "#         result = []\n",
    "\n",
    "#         def walk_list(ol, prefix=\"\"):\n",
    "#             items = ol.find_all(\"li\", recursive=False)\n",
    "#             for idx, li in enumerate(items, 1):\n",
    "#                 number = f\"{prefix}.{idx}\" if prefix else str(idx)\n",
    "#                 li_copy = BeautifulSoup(str(li), \"html.parser\")\n",
    "#                 for sublist in li_copy.find_all(\"ol\"):\n",
    "#                     sublist.decompose()\n",
    "#                 text = li_copy.get_text(separator=\" \", strip=True)\n",
    "#                 result.append(f\"{number}. {text}\")\n",
    "\n",
    "#                 sub_ol = li.find(\"ol\")\n",
    "#                 if sub_ol:\n",
    "#                     walk_list(sub_ol, number)\n",
    "\n",
    "#         top_ol = content.find(\"ol\")\n",
    "#         if top_ol:\n",
    "#             walk_list(top_ol)\n",
    "#         else:\n",
    "#             print(\"⚠️ Keine <ol> gefunden!\")\n",
    "\n",
    "#         return \"\\n\".join(result)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Fehler beim Scrapen CommonPaper: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "\n",
    "# # 3. Scraper für Fakturia-Verträge\n",
    "# def scrape_html_fakturia(url):\n",
    "#     try:\n",
    "#         headers = {\n",
    "#             \"User-Agent\": (\n",
    "#                 \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "#                 \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "#                 \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "#             )\n",
    "#         }\n",
    "#         response = requests.get(url, headers=headers)\n",
    "#         response.raise_for_status()\n",
    "\n",
    "#         soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#         content = soup.find(\"div\", class_=\"entry-content-wrapper\")\n",
    "#         if not content:\n",
    "#             print(\"⚠️ Fakturia: Kein Hauptbereich gefunden.\")\n",
    "#             return \"\"\n",
    "\n",
    "#         result = []\n",
    "#         section = \"\"\n",
    "\n",
    "#         for elem in content.find_all([\"h2\", \"p\"]):\n",
    "#             text = re.sub(r'\\s+', ' ', elem.get_text(separator=\" \", strip=True))\n",
    "\n",
    "#             if elem.name == \"h2\":\n",
    "#                 if section:\n",
    "#                     result.append(section.strip())\n",
    "#                 section = text + \"\\n\"\n",
    "#             elif elem.name == \"p\":\n",
    "#                 if re.match(r'^\\d+\\.\\d+', text):\n",
    "#                     section += text + \" \"\n",
    "#                 else:\n",
    "#                     section += text + \"\\n\"\n",
    "\n",
    "#         if section:\n",
    "#             result.append(section.strip())\n",
    "\n",
    "#         for marker in [\"Copyright OSB Alliance e.V.\", \"gemäß CC BY\", \"Version 1/2015\"]:\n",
    "#             if marker in result[-1]:\n",
    "#                 result[-1] = result[-1].split(marker)[0].strip()\n",
    "#                 break\n",
    "\n",
    "#         return \"\\n\\n\".join(result)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Fehler beim Scrapen Fakturia: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "\n",
    "# # 4. Scraper für Mitratech-Verträge\n",
    "# def scrape_html_mitratech(url):\n",
    "#     try:\n",
    "#         headers = {\n",
    "#             \"User-Agent\": (\n",
    "#                 \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "#                 \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "#                 \"Chrome/122.0.0.0 Safari/537.36\"\n",
    "#             )\n",
    "#         }\n",
    "#         response = requests.get(url, headers=headers)\n",
    "#         response.raise_for_status()\n",
    "\n",
    "#         soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#         for tag in soup([\"script\", \"style\", \"header\", \"footer\", \"nav\", \"form\", \"noscript\"]):\n",
    "#             tag.decompose()\n",
    "\n",
    "#         main = soup.find(\"main\") or soup\n",
    "#         found = False\n",
    "#         blocks = []\n",
    "\n",
    "#         for el in main.find_all([\"h1\", \"h2\", \"h3\", \"p\", \"li\", \"ol\", \"ul\"]):\n",
    "#             text = el.get_text(separator=\" \", strip=True)\n",
    "#             if not text:\n",
    "#                 continue\n",
    "\n",
    "#             if not found and text.startswith(\"1. Allgemeines\"):\n",
    "#                 found = True\n",
    "#                 blocks.append(text)\n",
    "#                 continue\n",
    "\n",
    "#             if found and el.name in [\"h1\", \"h2\", \"h3\"] and \"Begriffsbestimmungen\" in text:\n",
    "#                 break\n",
    "\n",
    "#             if found:\n",
    "#                 blocks.append(text)\n",
    "\n",
    "#         return \"\\n\\n\".join(blocks).strip()\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Fehler beim Scrapen Mitratech: {e}\")\n",
    "#         return \"\"\n",
    "\n",
    "\n",
    "# # Automatische Auswahl je nach URL\n",
    "# def scrape_contract_auto(url):\n",
    "    url_lc = url.lower()\n",
    "    if \"commonpaper.com\" in url_lc:\n",
    "        return scrape_html_commonpaper(url)\n",
    "    elif \"fakturia.de\" in url_lc:\n",
    "        return scrape_html_fakturia(url)\n",
    "    elif \"mitratech.com\" in url_lc or \"alyne.com\" in url_lc:\n",
    "        return scrape_html_mitratech(url)\n",
    "    else:\n",
    "        return scrape_html_standard(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25180f9",
   "metadata": {},
   "source": [
    "## 1.2 Reading in PDF, DOCX and JSON\n",
    "\n",
    "Since we realised that all the files are delivered in different formats and therefore trying to automate the reading process won´t be really sucsesfull, since you have to write a new function for every document we stopped that approach. If we would continue like this we would have to write a seperate function for each document, considering the slight differences each document comes with.\n",
    "\n",
    "Since this would consume a lot of time and is not very efficient as prooven by the HTML example we decided to simply copy all the relevant DOCX, PDF and JSON files into TXT files manually. This is because it is way easier for us to read in txt files that are all in the same format.\n",
    "\n",
    "This project is about NLP and not so much about building automated scraping tools. Therefore we think this apporach is reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456d6898",
   "metadata": {},
   "source": [
    "**TXT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a644a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Funktion zum einlesen von .txt files\n",
    "# def read_txt_file(file_path):\n",
    "#     try:\n",
    "#         with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#             content = file.read()\n",
    "#         return content\n",
    "#     except Exception as e:\n",
    "#         print(f\"Fehler beim Einlesen der Datei: {e}\")\n",
    "#         return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d8368",
   "metadata": {},
   "source": [
    "**Read mapping**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c83b74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "excel_path = Path(\"../data/input_mapping/Mappingliste_Verträge.xlsx\")\n",
    "df = pd.read_excel(excel_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833cc52c",
   "metadata": {},
   "source": [
    "**Neue Spalte Content und Filetype in DF erzeugen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1477bb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Content' not in df.columns:\n",
    "    df['Content'] = \"\"\n",
    "\n",
    "if 'FileType' not in df.columns:\n",
    "    df['FileType'] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad208b3",
   "metadata": {},
   "source": [
    "**TxT files und HTML links automatisiert in Data Frame einlesen und als pickle file speichern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6fb88ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m         file_type = \u001b[33m\"\u001b[39m\u001b[33mTXT\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     texts.append(\u001b[43mscrape_contract_auto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m     24\u001b[39m         file_type = \u001b[33m\"\u001b[39m\u001b[33mHTML\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/functions/function_contract_read_in.py:196\u001b[39m, in \u001b[36mscrape_contract_auto\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m    194\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m scrape_html_mitratech(url)\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscrape_html_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/functions/function_contract_read_in.py:16\u001b[39m, in \u001b[36mscrape_html_standard\u001b[39m\u001b[34m(url)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      9\u001b[39m     headers = {\n\u001b[32m     10\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: (\n\u001b[32m     11\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMozilla/5.0 (Windows NT 10.0; Win64; x64) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     14\u001b[39m         )\n\u001b[32m     15\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     response = \u001b[43mrequests\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     response.encoding = \u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     18\u001b[39m     response.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/.venv/lib/python3.11/site-packages/requests/api.py:73\u001b[39m, in \u001b[36mget\u001b[39m\u001b[34m(url, params, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget\u001b[39m(url, params=\u001b[38;5;28;01mNone\u001b[39;00m, **kwargs):\n\u001b[32m     63\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request.\u001b[39;00m\n\u001b[32m     64\u001b[39m \n\u001b[32m     65\u001b[39m \u001b[33;03m    :param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     70\u001b[39m \u001b[33;03m    :rtype: requests.Response\u001b[39;00m\n\u001b[32m     71\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mget\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/.venv/lib/python3.11/site-packages/requests/api.py:59\u001b[39m, in \u001b[36mrequest\u001b[39m\u001b[34m(method, url, **kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# By using the 'with' statement we are sure the session is closed, thus we\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# avoid leaving sockets open which can trigger a ResourceWarning in some\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# cases, and look like a memory leak in others.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sessions.Session() \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/.venv/lib/python3.11/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/.venv/lib/python3.11/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/.venv/lib/python3.11/site-packages/requests/adapters.py:667\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    664\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    666\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m667\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    679\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    682\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/.venv/lib/python3.11/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/.venv/lib/python3.11/site-packages/urllib3/connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    752\u001b[39m     sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m     server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m    755\u001b[39m     tls_in_tls = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/.venv/lib/python3.11/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/.venv/lib/python3.11/site-packages/urllib3/util/connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[32m     75\u001b[39m err = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Basisordner für lokale Vertragsdateien\n",
    "base_path = Path(\"../data/verträge/verträge_txt\")\n",
    "\n",
    "# Iteration über die Mapping-Tabelle\n",
    "for idx, row in df.iterrows():\n",
    "    mapping_field = row['Mapping']\n",
    "    content = \"\"\n",
    "    file_type = \"\"\n",
    "\n",
    "    if pd.notna(mapping_field):\n",
    "        mappings = [m.strip() for m in mapping_field.split(';')]\n",
    "        texts = []\n",
    "\n",
    "        for i, mapping in enumerate(mappings):\n",
    "            if mapping.endswith('.txt'):\n",
    "                filename = Path(mapping).name  # nur Dateiname\n",
    "                filepath = base_path / filename\n",
    "                texts.append(read_txt_file(filepath))\n",
    "                if i == 0:\n",
    "                    file_type = \"TXT\"\n",
    "            else:\n",
    "                texts.append(scrape_contract_auto(mapping))\n",
    "                if i == 0:\n",
    "                    file_type = \"HTML\"\n",
    "\n",
    "        content = \"\\n\\n\".join(texts)\n",
    "\n",
    "    df.at[idx, 'Content'] = content\n",
    "    df.at[idx, 'FileType'] = file_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1b0d70",
   "metadata": {},
   "source": [
    "**Englische Texte übersetzen**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f830ab67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# .env laden für API-Key\n",
    "load_dotenv()\n",
    "\n",
    "# src-Ordner zum Pfad hinzufügen, damit translate.py importiert werden kann\n",
    "sys.path.append(str(Path(\"..\") / \"src\"))\n",
    "\n",
    "# Funktion importieren\n",
    "from translate import translate_dataframe\n",
    "# Übersetzung auf Texte mit Sprache 'EN' anwenden\n",
    "df_translated = translate_dataframe(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b4750",
   "metadata": {},
   "source": [
    "**Fertige input files als pickle file speichern**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4a2805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabeordner und -datei\n",
    "output_pickle_path = Path(\"../data/data_scraped_input.pkl\")\n",
    "# Ergebnisse speichern\n",
    "df_translated.to_pickle(output_pickle_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4971cef9",
   "metadata": {},
   "source": [
    "# 2. Data cleaning\n",
    "## 2.1 Data Loading and Initial Structuring\n",
    "\n",
    "In the first step, we load the data from the pickled dataset produced by the data_script_input. After reading in the contracts, each entry in our custom dataset is enriched with key metadata, including:\n",
    "\n",
    "- the source of the data\n",
    "- the document type\n",
    "- a mapping to the original source website\n",
    "- the language of the document\n",
    "- the full contract content\n",
    "- and the file type\n",
    "\n",
    "This step ensures that all contracts are consistently structured and traceable back to their origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "451eb061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Kategorie</th>\n",
       "      <th>Quelle/Organisation</th>\n",
       "      <th>Dokumententyp</th>\n",
       "      <th>Mapping</th>\n",
       "      <th>Sprache</th>\n",
       "      <th>Content</th>\n",
       "      <th>FileType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Verbände / Templates</td>\n",
       "      <td>IT-Recht Hannover</td>\n",
       "      <td>Muster SaaS-Vertrag</td>\n",
       "      <td>https://it-rechthannover.de/IT-Muster/SaaS-Ver...</td>\n",
       "      <td>DE</td>\n",
       "      <td>§ 1 Vertragsgegenstand 1.1 Der Anbieter stellt...</td>\n",
       "      <td>HTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Verbände / Templates</td>\n",
       "      <td>3H Solutions AG</td>\n",
       "      <td>Standard-Vertragsbedingungen SaaS</td>\n",
       "      <td>Templates_3H_Solutions_AG_18-06_SaaS-Cloudsoft...</td>\n",
       "      <td>DE</td>\n",
       "      <td>Standard-Vertragsbedingungen\\nSaaS- und Clouds...</td>\n",
       "      <td>TXT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Verbände / Templates</td>\n",
       "      <td>Common Paper</td>\n",
       "      <td>Cloud Service Agreement</td>\n",
       "      <td>https://commonpaper.com/standards/cloud-servic...</td>\n",
       "      <td>EN</td>\n",
       "      <td>1. Service\\n1.1. Access and Use. During the Su...</td>\n",
       "      <td>HTML</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Öffentlich zugängliche Verträge großer SaaS-An...</td>\n",
       "      <td>SAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SaaS_SAP_Service_Level_Agreement.txt</td>\n",
       "      <td>DE</td>\n",
       "      <td>SERVICE-LEVEL-VEREINBARUNG FÜR PRIVATE CLOUD E...</td>\n",
       "      <td>TXT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Öffentlich zugängliche Verträge großer SaaS-An...</td>\n",
       "      <td>SAP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Saas_SAP_General_Terms.txt</td>\n",
       "      <td>DE</td>\n",
       "      <td>ALLGEMEINE GESCHÄFTSBEDINGUNGEN FÜR CLOUD SERV...</td>\n",
       "      <td>TXT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Kategorie Quelle/Organisation  \\\n",
       "0                               Verbände / Templates   IT-Recht Hannover   \n",
       "1                               Verbände / Templates     3H Solutions AG   \n",
       "2                               Verbände / Templates        Common Paper   \n",
       "3  Öffentlich zugängliche Verträge großer SaaS-An...                 SAP   \n",
       "4  Öffentlich zugängliche Verträge großer SaaS-An...                 SAP   \n",
       "\n",
       "                       Dokumententyp  \\\n",
       "0                Muster SaaS-Vertrag   \n",
       "1  Standard-Vertragsbedingungen SaaS   \n",
       "2            Cloud Service Agreement   \n",
       "3                                NaN   \n",
       "4                                NaN   \n",
       "\n",
       "                                             Mapping Sprache  \\\n",
       "0  https://it-rechthannover.de/IT-Muster/SaaS-Ver...      DE   \n",
       "1  Templates_3H_Solutions_AG_18-06_SaaS-Cloudsoft...      DE   \n",
       "2  https://commonpaper.com/standards/cloud-servic...      EN   \n",
       "3               SaaS_SAP_Service_Level_Agreement.txt      DE   \n",
       "4                         Saas_SAP_General_Terms.txt      DE   \n",
       "\n",
       "                                             Content FileType  \n",
       "0  § 1 Vertragsgegenstand 1.1 Der Anbieter stellt...     HTML  \n",
       "1  Standard-Vertragsbedingungen\\nSaaS- und Clouds...      TXT  \n",
       "2  1. Service\\n1.1. Access and Use. During the Su...     HTML  \n",
       "3  SERVICE-LEVEL-VEREINBARUNG FÜR PRIVATE CLOUD E...      TXT  \n",
       "4  ALLGEMEINE GESCHÄFTSBEDINGUNGEN FÜR CLOUD SERV...      TXT  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_all_contracts = pd.read_pickle(\"../data/data_scraped_input.pkl\")\n",
    "display(df_all_contracts.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56029cea",
   "metadata": {},
   "source": [
    "**Example of Contract Content:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4395087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vertragsbedingungen SaaS-Vertrag\n",
      "der TA Triumph-Adler Gruppe (Stand 01/2021)\n",
      "Vertragsbedingungen SaaS-Vertrag der TA Triumph-Adler Gruppe\n",
      "(Stand 01/2021) – Seite 1 von 5\n",
      "1. Vertragsgegenstand, Anwendungsbereich\n",
      "1.1. Diese „Vertragsbedingungen SaaS-Vertrag TA Triumph-Adler Gruppe“\n",
      "(„Vertragsbedingungen“) sind Bestandteil des zwischen Auftragnehmer und\n",
      "Auftraggeber (gemeinsam „Parteien“) abgeschlossenen Software as a\n",
      "Service-Vertrags („SaaS-Vertrag“).\n",
      "1.2. Bestandteil des SaaS-Vertrags sind je nach Vereinbarung im SaaS-Vertrag:\n",
      "a) die entgeltliche Überlassung folgender Objekte:\n",
      "- Softwareanwendung mittels Internet, soweit keine anderweitige\n",
      "Telekommunikation ausdrücklich vereinbart wurde („Services“),\n",
      "- und/oder\n",
      "- Software („Vertragssoftware“) einschließlich der zugehörigen Beschreibung\n",
      "der technischen Funktionalität, des Betriebs, der Installation und der Nutzung,\n",
      "b) die Erbringung von Serviceleistungen an den Services,\n",
      "c) die Erbringung von Softwarepflege- und -supportleistungen\n",
      "(„SPS-...\n"
     ]
    }
   ],
   "source": [
    "df = df_all_contracts\n",
    "print(df.iloc[13, 5][:1000] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ce569",
   "metadata": {},
   "source": [
    "## 2.2 Filter and Select Data \n",
    "For further data processing, we retain only the content and contract columns, as these contain the essential information for our analysis.\n",
    "\n",
    "Additionally, we focus exclusively on contracts written in German, since the goal is to develop a German-language contract checker. Filtering by language at this stage ensures consistency and avoids noise from multilingual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58db68e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter df to relevant contracts\n",
    "df_relevant = df[#(df['Kategorie'] == \"kleinere SaaS-Anbieter (Hauptgruppe)\") & \n",
    "                    (df['Sprache'] == \"DE\") #& (df['Quelle/Organisation'] != \"Comarch ERP XT\"\t)\n",
    "                    ]\n",
    "df_relevant = df_relevant.iloc[:,[5]]\n",
    "df_relevant.columns = ['content']\n",
    "df_relevant[\"contract\"] = range(1, df_relevant.shape[0] + 1)\n",
    "df_relevant = df_relevant[['contract', 'content']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2af80c",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "921eaed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   contract                                            content\n",
      "0         1  § 1 Vertragsgegenstand 1.1 Der Anbieter stellt...\n",
      "1         2  Standard-Vertragsbedingungen\\nSaaS- und Clouds...\n",
      "3         3  SERVICE-LEVEL-VEREINBARUNG FÜR PRIVATE CLOUD E...\n",
      "4         4  ALLGEMEINE GESCHÄFTSBEDINGUNGEN FÜR CLOUD SERV...\n",
      "5         5  SUPPORT SCHEDULE FÜR CLOUD SERVICES\\nDieses Su...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_relevant.head())\n",
    "with open(\"../data/data_scraped_input_relevant.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_relevant, f)\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "df_relevant.to_excel(\"../data/data_scraped_input_relevant.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d21ce9",
   "metadata": {},
   "source": [
    "## 2.3 Slicing \n",
    "Since we aim to analyze individual sections rather than entire contracts, the next step is to split the contract texts into smaller segments. Specifically, we divide each contract into multiple rows, first by paragraphs, and then by subsections within each paragraph. This segmentation makes it possible to process and classify specific parts of the contract more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "354dd455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_paragraphs_and_sections(row, col='content', contract_col='contract', print_steps = False):\n",
    "#     import re\n",
    "\n",
    "#     text = row[col]\n",
    "#     if contract_col ==None:\n",
    "#         contract_id= 1\n",
    "#     else:\n",
    "#         contract_id = row['contract']\n",
    "#     lines = text.splitlines()\n",
    "#     paragraphs = []\n",
    "#     current_para_lines = []\n",
    "#     current_para_number = 0\n",
    "#     current_para_match = None\n",
    "#     match_pat_type_1 = True\n",
    "#     match_pat_type_2 = True\n",
    "#     match_pat_type_3 = True\n",
    "#     para_mode = None\n",
    "\n",
    "#     # 1. extract paragraphs\n",
    "\n",
    "#     for line in lines:\n",
    "#         line = line.strip()\n",
    "#         if not line:\n",
    "#             continue\n",
    "\n",
    "#         search_for = str(int(current_para_number) + 1)\n",
    "\n",
    "        \n",
    "#         if para_mode == \"symbol\":\n",
    "#             if search_for:\n",
    "#                 match_main = re.match(rf'§\\s*{search_for}(?!\\d)', line)\n",
    "#         elif para_mode == \"number\":\n",
    "#             if search_for:\n",
    "#                 match_main = re.match(rf'\\b{search_for}\\.(?!\\d)', line)\n",
    "#         else:\n",
    "#             # Noch kein Modus festgelegt: beides probieren\n",
    "#             match_main = re.match(rf'(§\\s*(\\d+))(?!\\d)|\\b(\\d+)\\.(?!\\d)', line)\n",
    "#             if match_main:\n",
    "#                 if match_main.group(1):  # § X\n",
    "#                     para_mode = \"symbol\"\n",
    "#                 elif match_main.group(3):  # X.\n",
    "#                     para_mode = \"number\"\n",
    "       \n",
    "\n",
    "#         if match_main:\n",
    "#             if current_para_lines:\n",
    "#                 paragraphs.append((current_para_number, ' '.join(current_para_lines), current_para_match))\n",
    "#             current_para_number = match_main.group(0).strip().lstrip('§').rstrip('.').strip()  # e.g § 2 lorem ipsum --> 2\n",
    "#             current_para_lines = [line]                                                        # e.g § 2 lorem ipsum --> § 2 lorem impsum\n",
    "#             current_para_match = match_main.group(0).strip()                                   # e.g § 2 lorem ipsum --> § 2\n",
    "#         elif current_para_lines:\n",
    "#             current_para_lines.append(line)\n",
    "\n",
    "#     if current_para_lines:\n",
    "#         paragraphs.append((current_para_number, ' '.join(current_para_lines), current_para_match))\n",
    "\n",
    "#     rows = []\n",
    "#     seen_sections = set()  # (contract_id, para_num, section_id)\n",
    "\n",
    "#     for para_num, para_text, para_match in paragraphs:\n",
    "        \n",
    "#         if para_mode == \"number\":\n",
    "#             matches = list(re.finditer(rf'(?:(?<=\\s)|(?<=^))({para_num}\\.\\d{{1}})(?![\\dA-Za-z])|\\((\\d+)\\)', para_text))\n",
    "#         if para_mode == \"symbol\":\n",
    "#             matches = list(re.finditer(rf'(?:(?<=\\s)|(?<=^))({para_num}\\.\\d{{1}})(?![\\dA-Za-z])|\\((\\d+)\\)|\\b(\\d+)\\.(?!\\d)', para_text))\n",
    "\n",
    "#         if print_steps:\n",
    "#             print(para_num)\n",
    "#             print(seen_sections)\n",
    "#             print(para_text)\n",
    "#             print(matches)\n",
    "            \n",
    "\n",
    "#         if not matches:\n",
    "#             rows.append({\n",
    "#                 'contract': contract_id,\n",
    "#                 'paragraph': para_match,\n",
    "#                 'paragraph_content': para_text.strip(),\n",
    "#                 'section': \"no sections use paragraph\",\n",
    "#                 'section_content': para_text.strip()\n",
    "#             })\n",
    "#             continue\n",
    "\n",
    "#         positions = []\n",
    "#         last_section_number = 0\n",
    "        \n",
    "\n",
    "#         for match in matches:\n",
    "#             # hole entweder dezimale section (z. B. 1.1) oder Klammer-section (z. B. (1))\n",
    "#             section_id = match.group(1) or match.group(2) or match.group(3)\n",
    "#             start = match.start()\n",
    "\n",
    "#             # Unterscheide die Formate\n",
    "#             if match.group(1) and match_pat_type_1:  # Dezimal: z. B. \"1.5\"\n",
    "#                 try:\n",
    "#                     section_suffix = int(section_id.split(\".\")[1])\n",
    "#                 except (IndexError, ValueError):\n",
    "#                     continue  # überspringen bei Fehler\n",
    "#                 match_pat_type_2 = False # If first pattern type detected only look for this one\n",
    "#                 match_pat_type_3 = False\n",
    "\n",
    "#                 # verbiete z. B. \"1.50\"\n",
    "#                # if re.match(rf'{para_num}\\.\\d{{2,}}$', section_id):\n",
    "#                 #    continue\n",
    "\n",
    "#             elif match.group(2) and match_pat_type_2:  # Klammer: z. B. \"(2)\"\n",
    "#                 try:\n",
    "#                     section_suffix = int(section_id.strip(\"()\")) \n",
    "#                     section_id = f'({section_suffix})'  # Einheitliches Format für Ausgabe\n",
    "#                 except ValueError:\n",
    "#                     continue\n",
    "#                 match_pat_type_1 = False # If second pattern type detected only look for this one\n",
    "#                 match_pat_type_3 = False\n",
    "#             elif para_mode == \"symbol\" and match.group(3) and match_pat_type_3:  # 1. (nur bei mode=symbol)\n",
    "#                 if print_steps:\n",
    "#                     print(section_id)\n",
    "#                 try:\n",
    "#                     section_suffix = int(section_id.split(\".\")[0])\n",
    "#                     section_id = f'{section_suffix}.'  # für Klarheit\n",
    "#                 except ValueError:\n",
    "#                     continue\n",
    "#                 match_pat_type_1 = False # If third pattern type detected only look for this one\n",
    "#                 match_pat_type_2 = False\n",
    "\n",
    "#             else:\n",
    "#                 continue  # kein gültiges Format\n",
    "\n",
    "#             # Nur nächste Zahl zulassen\n",
    "#             if last_section_number != 0 and section_suffix != last_section_number + 1:\n",
    "#                 continue\n",
    "\n",
    "#             section_key = (contract_id, para_num, section_id)\n",
    "#             if section_key in seen_sections:\n",
    "#                 continue\n",
    "\n",
    "#             seen_sections.add(section_key)\n",
    "#             positions.append((start, section_id))\n",
    "#             last_section_number = section_suffix\n",
    "\n",
    "\n",
    "#         # Add end position\n",
    "#         positions.append((len(para_text), None))\n",
    "#         positions = sorted(positions)\n",
    "#         if print_steps:\n",
    "#             print(f'positions = {positions}')\n",
    "#             print('###########')\n",
    "\n",
    "#         for i in range(len(positions) - 1):\n",
    "#             start_pos = positions[i][0]\n",
    "#             end_pos = positions[i + 1][0]\n",
    "#             section_id = positions[i][1]\n",
    "#             section_text = para_text[start_pos:end_pos].strip()\n",
    "\n",
    "#             rows.append({\n",
    "#                 'contract': contract_id,\n",
    "#                 'paragraph': para_match,\n",
    "#                 'paragraph_content': para_text.strip(),\n",
    "#                 'section': section_id,\n",
    "#                 'section_content': section_text\n",
    "#             })\n",
    "\n",
    "#     return rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf2b996",
   "metadata": {},
   "source": [
    "**New structure**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dafc517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [{'contract': 1, 'paragraph': '§ 1', 'paragrap...\n",
      "1    [{'contract': 2, 'paragraph': '§ 1', 'paragrap...\n",
      "3    [{'contract': 3, 'paragraph': '1.', 'paragraph...\n",
      "4    [{'contract': 4, 'paragraph': '1.', 'paragraph...\n",
      "5    [{'contract': 5, 'paragraph': '1.', 'paragraph...\n",
      "dtype: object\n",
      "               \\#########/\n",
      "                \\#######/\n",
      "                 \\#####/\n",
      "                  \\###/\n",
      "                   \\#/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>section</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>(1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>(2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>26</td>\n",
       "      <td>19.</td>\n",
       "      <td>19.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1372</th>\n",
       "      <td>26</td>\n",
       "      <td>19.</td>\n",
       "      <td>19.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1373</th>\n",
       "      <td>26</td>\n",
       "      <td>19.</td>\n",
       "      <td>19.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1374</th>\n",
       "      <td>26</td>\n",
       "      <td>19.</td>\n",
       "      <td>19.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1375</th>\n",
       "      <td>26</td>\n",
       "      <td>19.</td>\n",
       "      <td>19.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1376 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      contract paragraph section\n",
       "0            1       § 1     1.1\n",
       "1            1       § 1     1.2\n",
       "2            1       § 1     1.3\n",
       "3            2       § 1     (1)\n",
       "4            2       § 1     (2)\n",
       "...        ...       ...     ...\n",
       "1371        26       19.    19.2\n",
       "1372        26       19.    19.3\n",
       "1373        26       19.    19.4\n",
       "1374        26       19.    19.5\n",
       "1375        26       19.    19.6\n",
       "\n",
       "[1376 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_exploded = df_relevant.apply(extract_paragraphs_and_sections, axis=1)\n",
    "print(df_exploded.head())\n",
    "flattened_rows = list(chain.from_iterable(df_exploded))\n",
    "df_structured = pd.DataFrame(flattened_rows)\n",
    "print(\"               \\\\#########/\")\n",
    "print(\"                \\\\#######/\")\n",
    "print(\"                 \\\\#####/\")\n",
    "print(\"                  \\\\###/\")\n",
    "print(\"                   \\\\#/\")\n",
    "\n",
    "\n",
    "display(df_structured[[\"contract\",\"paragraph\",\"section\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2036108",
   "metadata": {},
   "source": [
    "In a second step, we aim to extract paragraph titles directly from the paragraph content. To achieve this, we use regular expressions (regex) that match common patterns typically found at the beginning of legal paragraphs—such as numbered clauses, keywords like \"Der\", \"Ein\", or \"Eine\", or capitalized phrases.\n",
    "\n",
    "Since this is not working totally well and quite often ni recognizable pattern is found, we apply a fallback strategy: we extract a short snippet from the beginning of the paragraph (e.g., the first few words or until the first full sentence) to serve as a temporary title.\n",
    "\n",
    "This ensures that each paragraph receives a consistent and descriptive title, even if the document does not explicitly define one. These titles are useful for labeling, classification, and structuring contract documents for downstream tasks. Furthermore the Paragraph tag and the number is removed. \n",
    "\n",
    "Since we want the later algorithm to focus on the content rather than focusing on the title we also remove the title from the content of the prargraph as well as drom the content of the section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87421b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_title_fixed(group):\n",
    "#     import re\n",
    "#     paragraph_text = group['paragraph_content'].iloc[0]\n",
    "#     section_texts = group['section_content'].tolist()\n",
    "\n",
    "#     # No Sections (single paragraph)\n",
    "#     if len(section_texts) == 1 and group['section'].iloc[0] == \"no sections use paragraph\":\n",
    "#         # find sentence end\n",
    "#         match = re.search(r'\\b(Der|Die|Das|Es|Ein|Eine)\\s+[A-ZÄÖÜ][a-zäöü]+\\b', paragraph_text)\n",
    "#         if match:\n",
    "#             title = paragraph_text[:match.start()].strip()\n",
    "#         else:\n",
    "#             # Fallback: to first verb or 8 words\n",
    "#             title = ' '.join(paragraph_text.split()[:8])\n",
    "#         return pd.Series([title] * len(group), index=group.index)\n",
    "\n",
    "#     # secction split\n",
    "#     for section in section_texts:\n",
    "#         paragraph_text = paragraph_text.replace(section, '')\n",
    "#     title = paragraph_text.strip()\n",
    "#     return pd.Series([title] * len(group), index=group.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0f2ea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n9/9rvxcg4d1d15_3wh3wq9f0l40000gn/T/ipykernel_28938/3127144179.py:1: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_structured['paragraph_title'] = df_structured.groupby(['contract', 'paragraph'], group_keys= False).apply(extract_title_fixed)\n"
     ]
    }
   ],
   "source": [
    "df_structured['paragraph_title'] = df_structured.groupby(['contract', 'paragraph'], group_keys= False).apply(extract_title_fixed)\n",
    "\n",
    "\n",
    "df_structured = df_structured[\n",
    "    ['contract', 'paragraph', 'paragraph_title', 'paragraph_content', 'section', 'section_content']\n",
    "]\n",
    "\n",
    "\n",
    "df_structured['paragraph_title'] = df_structured.apply(\n",
    "    lambda row: row['paragraph_title'].replace(row['paragraph'], '').strip() if pd.notnull(row['paragraph_title']) else '',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "\n",
    "df_structured[\"paragraph_content\"] = df_structured.apply(\n",
    "    lambda row: row[\"paragraph_content\"].replace(row['paragraph_title'], '').strip() if pd.notnull(row[\"paragraph_content\"]) else '',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_structured[\"section_content\"] = df_structured.apply(\n",
    "    lambda row: row[\"section_content\"].replace(row['paragraph_title'], '').strip() if pd.notnull(row[\"section_content\"]) else '',\n",
    "    axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c8a41",
   "metadata": {},
   "source": [
    "**New structure**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "623bc6f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>paragraph_title</th>\n",
       "      <th>paragraph_content</th>\n",
       "      <th>section</th>\n",
       "      <th>section_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>Vertragsgegenstand</td>\n",
       "      <td>§ 1  1.1 Der Anbieter stellt dem Kunden die So...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1 Der Anbieter stellt dem Kunden die Softwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>Vertragsgegenstand</td>\n",
       "      <td>§ 1  1.1 Der Anbieter stellt dem Kunden die So...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2 Die Nutzung umfasst die Bereitstellung von...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>Vertragsgegenstand</td>\n",
       "      <td>§ 1  1.1 Der Anbieter stellt dem Kunden die So...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3 Der Kunde erhält ausschließlich das vertra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>Vertragsgegenstand</td>\n",
       "      <td>§ 1  (1) Dieser Software-as-a-Service-Vertrag ...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>(1) Dieser Software-as-a-Service-Vertrag ist a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>Vertragsgegenstand</td>\n",
       "      <td>§ 1  (1) Dieser Software-as-a-Service-Vertrag ...</td>\n",
       "      <td>(2)</td>\n",
       "      <td>(2) Die Software wird vom Anbieter als webbasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contract paragraph     paragraph_title  \\\n",
       "0         1       § 1  Vertragsgegenstand   \n",
       "1         1       § 1  Vertragsgegenstand   \n",
       "2         1       § 1  Vertragsgegenstand   \n",
       "3         2       § 1  Vertragsgegenstand   \n",
       "4         2       § 1  Vertragsgegenstand   \n",
       "\n",
       "                                   paragraph_content section  \\\n",
       "0  § 1  1.1 Der Anbieter stellt dem Kunden die So...     1.1   \n",
       "1  § 1  1.1 Der Anbieter stellt dem Kunden die So...     1.2   \n",
       "2  § 1  1.1 Der Anbieter stellt dem Kunden die So...     1.3   \n",
       "3  § 1  (1) Dieser Software-as-a-Service-Vertrag ...     (1)   \n",
       "4  § 1  (1) Dieser Software-as-a-Service-Vertrag ...     (2)   \n",
       "\n",
       "                                     section_content  \n",
       "0  1.1 Der Anbieter stellt dem Kunden die Softwar...  \n",
       "1  1.2 Die Nutzung umfasst die Bereitstellung von...  \n",
       "2  1.3 Der Kunde erhält ausschließlich das vertra...  \n",
       "3  (1) Dieser Software-as-a-Service-Vertrag ist a...  \n",
       "4  (2) Die Software wird vom Anbieter als webbasi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_structured.head())\n",
    "output_pickle_path = Path(\"../data/data_structured.pkl\")\n",
    "# Ergebnisse speichern\n",
    "df_structured.to_pickle(output_pickle_path)\n",
    "df_structured.to_excel(\"../data/data_structured.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e6ea8e",
   "metadata": {},
   "source": [
    "## 2.3 Cleaning\n",
    "\n",
    "In the next step, we focused on cleaning and normalizing the core of the dataset. For this purpose, we implemented a flexible function that allows us to experiment with various cleaning strategies via parameters. These options include:\n",
    "\n",
    "- Removing all paragraph markers (e.g., “§”, “1.2”)\n",
    "- Converting all text to lowercase\n",
    "- Stripping HTML tags\n",
    "- Removing numbers\n",
    "- Removing punctuation\n",
    "- Reducing multiple whitespaces to a single space\n",
    "- Removing short words (e.g., ≤ 2 characters)\n",
    "- Removing known stopwords (using Gensim’s stopword library)\n",
    "- Applying stemming to reduce words to their root form\n",
    "\n",
    "We tested various combinations of these settings across multiple runs. The best results were achieved with all cleaning steps enabled, except for stemming, which tended to distort meaning too much in our context.\n",
    "\n",
    "Therefore, we adopted this configuration as our standard cleaning approach going forward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cab3e1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contract</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>paragraph_title</th>\n",
       "      <th>paragraph_content</th>\n",
       "      <th>section</th>\n",
       "      <th>section_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>Vertragsgegenstand</td>\n",
       "      <td>§ 1  1.1 Der Anbieter stellt dem Kunden die So...</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1 Der Anbieter stellt dem Kunden die Softwar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>Vertragsgegenstand</td>\n",
       "      <td>§ 1  1.1 Der Anbieter stellt dem Kunden die So...</td>\n",
       "      <td>1.2</td>\n",
       "      <td>1.2 Die Nutzung umfasst die Bereitstellung von...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>Vertragsgegenstand</td>\n",
       "      <td>§ 1  1.1 Der Anbieter stellt dem Kunden die So...</td>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3 Der Kunde erhält ausschließlich das vertra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>Vertragsgegenstand</td>\n",
       "      <td>§ 1  (1) Dieser Software-as-a-Service-Vertrag ...</td>\n",
       "      <td>(1)</td>\n",
       "      <td>(1) Dieser Software-as-a-Service-Vertrag ist a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>§ 1</td>\n",
       "      <td>Vertragsgegenstand</td>\n",
       "      <td>§ 1  (1) Dieser Software-as-a-Service-Vertrag ...</td>\n",
       "      <td>(2)</td>\n",
       "      <td>(2) Die Software wird vom Anbieter als webbasi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   contract paragraph     paragraph_title  \\\n",
       "0         1       § 1  Vertragsgegenstand   \n",
       "1         1       § 1  Vertragsgegenstand   \n",
       "2         1       § 1  Vertragsgegenstand   \n",
       "3         2       § 1  Vertragsgegenstand   \n",
       "4         2       § 1  Vertragsgegenstand   \n",
       "\n",
       "                                   paragraph_content section  \\\n",
       "0  § 1  1.1 Der Anbieter stellt dem Kunden die So...     1.1   \n",
       "1  § 1  1.1 Der Anbieter stellt dem Kunden die So...     1.2   \n",
       "2  § 1  1.1 Der Anbieter stellt dem Kunden die So...     1.3   \n",
       "3  § 1  (1) Dieser Software-as-a-Service-Vertrag ...     (1)   \n",
       "4  § 1  (1) Dieser Software-as-a-Service-Vertrag ...     (2)   \n",
       "\n",
       "                                     section_content  \n",
       "0  1.1 Der Anbieter stellt dem Kunden die Softwar...  \n",
       "1  1.2 Die Nutzung umfasst die Bereitstellung von...  \n",
       "2  1.3 Der Kunde erhält ausschließlich das vertra...  \n",
       "3  (1) Dieser Software-as-a-Service-Vertrag ist a...  \n",
       "4  (2) Die Software wird vom Anbieter als webbasi...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_structured = pd.read_pickle(\"../data/data_structured.pkl\")\n",
    "display(df_structured.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6d5939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_sections_and_paragraphs(\n",
    "#     text,\n",
    "#     remove_paragraph_markers=True,\n",
    "#     to_lower=True,\n",
    "#     remove_tags=True,\n",
    "#     remove_numbers=True,\n",
    "#     remove_punctuation=True,\n",
    "#     remove_extra_whitespace=True,\n",
    "#     strip_short_words=False,\n",
    "#     remove_stopwords =False,\n",
    "#     apply_stemming=False\n",
    "# ):\n",
    "#     if not isinstance(text, str):\n",
    "#         return \"\"\n",
    "\n",
    "#     if to_lower:\n",
    "#         text = text.lower()\n",
    "\n",
    "#     if remove_paragraph_markers:\n",
    "#         # Remove paragraph indicators like \"§ 1\", \"1.\", \"1.1\" etc.\n",
    "#         text = re.sub(r'^(§?\\s*\\d+[a-zA-Z]*[.)]?(\\s*\\(?\\d+[.)]?)?)', '', text)\n",
    "#         text = re.sub(r'\\(?\\b\\d{1,2}(\\.\\d{1,2})?\\)?', '', text)\n",
    "\n",
    "#     if remove_tags:\n",
    "#         text = strip_tags(text)\n",
    "\n",
    "#     if remove_numbers:\n",
    "#         text = strip_numeric(text)\n",
    "\n",
    "#     if remove_punctuation:\n",
    "#         text = strip_punctuation(text)\n",
    "\n",
    "#     if remove_extra_whitespace:\n",
    "#         text = strip_multiple_whitespaces(text)\n",
    "\n",
    "#     if strip_short_words:\n",
    "#         text = strip_short(text, minsize=3)\n",
    "\n",
    "#     if remove_stopwords:\n",
    "#         text = remove_stopwords(text, stopwords=STOPWORDS) # !!!!!! Englische Stopwörter  !!!!!!!\n",
    "\n",
    "#     if apply_stemming:\n",
    "#         text = stem_text(text)\n",
    "\n",
    "#     return text.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ab55a",
   "metadata": {},
   "source": [
    "**appling the function with deaults**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "729bd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_structured[\"clean_paragraph_content\"] = df_structured[\"paragraph_content\"].apply(clean_sections_and_paragraphs)\n",
    "df_structured[\"clean_section_content\"] = df_structured[\"section_content\"].apply(clean_sections_and_paragraphs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a0dace",
   "metadata": {},
   "source": [
    "Example clean paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "135938b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "der anbieter stellt dem kunden die software name der software zur verfügung die über eine cloud infrastruktur zugänglich ist die nutzung umfasst die b...\n"
     ]
    }
   ],
   "source": [
    "print(df_structured[\"clean_paragraph_content\"][0][:150] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7999fc",
   "metadata": {},
   "source": [
    "Example clean section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4bc1f0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "der anbieter stellt dem kunden die software name der software zur verfügung die über eine cloud infr...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df_structured[\"clean_section_content\"][0][:100] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ec4f1a",
   "metadata": {},
   "source": [
    "# 2.4 Visualizing Token Distributions with Word Clouds\n",
    "To gain a better understanding of the most common words and tokens used across the contracts—at the session and paragraph level—we plan to generate several word clouds.\n",
    "\n",
    "These visualizations will be based on different tokenization stages, including:\n",
    "\n",
    "- Raw tokenization (including punctuation and original casing),\n",
    "- Stemming, and\n",
    "- Lemmatization\n",
    "\n",
    "By comparing these different views, we aim to identify frequently used legal terms, recurring patterns, and potentially meaningful vocabulary that could inform our downstream tasks such as classification, clustering, or contract clause extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0be14ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1091 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e84764773f4816be5ba5a7a0bcb02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(), Output(), Output()), selected_index=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "nlp = en_core_web_sm.load()\n",
    "bert_uncased_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "mlparaphrase_tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "\n",
    "\n",
    "### paragraphs\n",
    "df_structured[\"paragraph_content_stemm\"]=df_structured['clean_paragraph_content'].apply(\n",
    "    lambda text: stem_text(text)\n",
    ")\n",
    "df_structured[\"paragraph_content_lemma\"]=df_structured['clean_paragraph_content'].apply(\n",
    "    lambda text: \" \".join([token.lemma_ for token in nlp(text) if not token.is_space])\n",
    ")\n",
    "df_structured[\"paragraph_content_token_bert\"]=df_structured['clean_paragraph_content'].apply(\n",
    "    lambda text: bert_uncased_tokenizer.tokenize(text)\n",
    ")\n",
    "\n",
    "df_structured[\"paragraph_content_token_mlp\"]=df_structured['clean_paragraph_content'].apply(\n",
    "    lambda text: mlparaphrase_tokenizer.tokenize(text)\n",
    ")\n",
    "\n",
    "### sections\n",
    "\n",
    "df_structured[\"section_content_stemm\"]=df_structured['clean_section_content'].apply(\n",
    "    lambda text: stem_text(text)\n",
    ")\n",
    "df_structured[\"paragraph_section_lemma\"]=df_structured['clean_section_content'].apply(\n",
    "    lambda text: \" \".join([token.lemma_ for token in nlp(text) if not token.is_space])\n",
    ")\n",
    "df_structured[\"paragraph_section_token_bert\"]=df_structured['clean_section_content'].apply(\n",
    "    lambda text: bert_uncased_tokenizer.tokenize(text)\n",
    ")\n",
    "\n",
    "df_structured[\"paragraph_section_token_mlp\"]=df_structured['clean_section_content'].apply(\n",
    "    lambda text: mlparaphrase_tokenizer.tokenize(text)\n",
    ")\n",
    "\n",
    "\n",
    "df_clean = df_structured.copy()\n",
    "\n",
    "columns_and_titles = [\n",
    "    (\"paragraph_content_stemm\", \"Paragraph – Stemmed\"),\n",
    "    (\"paragraph_content_lemma\", \"Paragraph – Lemmatized\"),\n",
    "    (\"paragraph_content_token_bert\", \"Paragraph – BERT Tokens\"),\n",
    "    (\"paragraph_content_token_mlp\", \"Paragraph – Mulitlingual Paraphrase Tokens\"),\n",
    "    (\"section_content_stemm\", \"Section – Stemmed\"),\n",
    "    (\"paragraph_section_lemma\", \"Section – Lemmatized\"),\n",
    "    (\"paragraph_section_token_bert\", \"Section – BERT Tokens\"),\n",
    "    (\"paragraph_section_token_mlp\", \"Section – Mulitlingual Paraphrase Tokens\"),\n",
    "]\n",
    "\n",
    "\n",
    "tab_contents = []\n",
    "\n",
    "for col, title in columns_and_titles:\n",
    "    output = widgets.Output()\n",
    "    with output:\n",
    "        # Join tokens for token-based columns, else join raw text\n",
    "        if \"token\" in col:\n",
    "            all_text = \" \".join([\" \".join(tokens) if isinstance(tokens, list) else str(tokens)\n",
    "                                for tokens in df_structured[col].dropna()])\n",
    "        else:\n",
    "            all_text = \" \".join(df_structured[col].dropna())\n",
    "\n",
    "        # Generate word cloud\n",
    "        wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate(all_text)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"Wordcloud – {title}\", fontsize=16)\n",
    "        plt.show()\n",
    "        plt.close() \n",
    "\n",
    "        tab_contents.append((title, output))\n",
    "\n",
    "# Tabs erzeugen\n",
    "\n",
    "tab_widget = widgets.Tab()\n",
    "tab_widget.children = [out for _, out in tab_contents]\n",
    "for idx, (name, _) in enumerate(tab_contents):\n",
    "    tab_widget.set_title(idx, name)\n",
    "\n",
    "display(tab_widget)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b3161",
   "metadata": {},
   "source": [
    "===> Save cleaned Contract content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "384c1e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/data_clean.pkl'  \n",
    "df_clean.to_pickle(file_path)\n",
    "df_clean.to_excel(\"../data/data_clean.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b626047",
   "metadata": {},
   "source": [
    "# 3. Labeling\n",
    "## 3.1 Catalog cleaning\n",
    "In the next step, our goal is to assign labels to as many of our contract sections as possible, based on a predefined requirement catalog.\n",
    "The requirement catalog consists of the following three components:\n",
    "\n",
    "- Paragraph Topic: This indicates which paragraph or general area of the contract the requirement refers to.\n",
    "- Section Topic: A short guiding question that describes the specific aspect or issue that should be addressed within that section of the contract.\n",
    "- Example Sentence: A concrete example taken from an actual SaaS contract that illustrates how this requirement might typically be formulated in legal language.\n",
    "\n",
    "This structured setup allows us to later match contract content to catalog entries based on thematic and semantic similarity.\n",
    "\n",
    "However, before we can map the requirement catalog to our contract data, we first need to clean the example phrases (reference texts) contained within the catalog itself. These examples need to go through the same preprocessing pipeline, such as lowercasing, punctuation removal, and stopword filtering, to ensure that the label mapping is accurate and consistent with the processed contract content.\n",
    "\n",
    "Only once both the contract data and the catalog examples are cleaned can we begin matching them effectively for label assignment.\n",
    "\n",
    "**<=== load the raw catalogue**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9318c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_topic</th>\n",
       "      <th>section_topic</th>\n",
       "      <th>example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Projektkosten &amp; Zahlungsmodalitäten</td>\n",
       "      <td>Sind sämtliche Kostenarten und -bestandteile (...</td>\n",
       "      <td>„Im Festpreis von 200.000 € sind sämtliche Lei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Projektkosten &amp; Zahlungsmodalitäten</td>\n",
       "      <td>Ist das Vergütungsmodell eindeutig festgelegt ...</td>\n",
       "      <td>„Der Kunde zahlt eine monatliche Pauschale von...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Projektkosten &amp; Zahlungsmodalitäten</td>\n",
       "      <td>Ist ein Zahlungsplan mit konkreten Fälligkeite...</td>\n",
       "      <td>„Die Vergütung ist in drei Raten zahlbar: 30% ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Projektkosten &amp; Zahlungsmodalitäten</td>\n",
       "      <td>Sind Währung, Rechnungsstellung, Zahlungsfrist...</td>\n",
       "      <td>„Alle Preise verstehen sich in Euro zuzüglich ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Projektkosten &amp; Zahlungsmodalitäten</td>\n",
       "      <td>Regelt der Vertrag den Umgang mit Nebenkosten ...</td>\n",
       "      <td>„Reise- und Übernachtungskosten werden nur ers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Sonstige wichtige Klauseln</td>\n",
       "      <td>Ist die anwendbare Rechtsordnung eindeutig ver...</td>\n",
       "      <td>„Dieser Vertrag unterliegt dem Recht der Bunde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Sonstige wichtige Klauseln</td>\n",
       "      <td>Ist ein Gerichtsstand für Streitigkeiten festg...</td>\n",
       "      <td>„Gerichtsstand für alle Streitigkeiten aus ode...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Sonstige wichtige Klauseln</td>\n",
       "      <td>Bei mehrsprachigen Verträgen: Ist festgelegt, ...</td>\n",
       "      <td>„Dieser Vertrag wird in deutscher und englisch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>Sonstige wichtige Klauseln</td>\n",
       "      <td>Enthält der Vertrag eine salvatorische Klausel...</td>\n",
       "      <td>„Sollte eine Bestimmung dieses Vertrages unwir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Sonstige wichtige Klauseln</td>\n",
       "      <td>Gibt es eine Regelung, was gilt bei höherer Ge...</td>\n",
       "      <td>„Keine der Parteien haftet für die Nichterfüll...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>76 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        paragraph_topic  \\\n",
       "0   Projektkosten & Zahlungsmodalitäten   \n",
       "1   Projektkosten & Zahlungsmodalitäten   \n",
       "2   Projektkosten & Zahlungsmodalitäten   \n",
       "3   Projektkosten & Zahlungsmodalitäten   \n",
       "4   Projektkosten & Zahlungsmodalitäten   \n",
       "..                                  ...   \n",
       "71           Sonstige wichtige Klauseln   \n",
       "72           Sonstige wichtige Klauseln   \n",
       "73           Sonstige wichtige Klauseln   \n",
       "74           Sonstige wichtige Klauseln   \n",
       "75           Sonstige wichtige Klauseln   \n",
       "\n",
       "                                        section_topic  \\\n",
       "0   Sind sämtliche Kostenarten und -bestandteile (...   \n",
       "1   Ist das Vergütungsmodell eindeutig festgelegt ...   \n",
       "2   Ist ein Zahlungsplan mit konkreten Fälligkeite...   \n",
       "3   Sind Währung, Rechnungsstellung, Zahlungsfrist...   \n",
       "4   Regelt der Vertrag den Umgang mit Nebenkosten ...   \n",
       "..                                                ...   \n",
       "71  Ist die anwendbare Rechtsordnung eindeutig ver...   \n",
       "72  Ist ein Gerichtsstand für Streitigkeiten festg...   \n",
       "73  Bei mehrsprachigen Verträgen: Ist festgelegt, ...   \n",
       "74  Enthält der Vertrag eine salvatorische Klausel...   \n",
       "75  Gibt es eine Regelung, was gilt bei höherer Ge...   \n",
       "\n",
       "                                              example  \n",
       "0   „Im Festpreis von 200.000 € sind sämtliche Lei...  \n",
       "1   „Der Kunde zahlt eine monatliche Pauschale von...  \n",
       "2   „Die Vergütung ist in drei Raten zahlbar: 30% ...  \n",
       "3   „Alle Preise verstehen sich in Euro zuzüglich ...  \n",
       "4   „Reise- und Übernachtungskosten werden nur ers...  \n",
       "..                                                ...  \n",
       "71  „Dieser Vertrag unterliegt dem Recht der Bunde...  \n",
       "72  „Gerichtsstand für alle Streitigkeiten aus ode...  \n",
       "73  „Dieser Vertrag wird in deutscher und englisch...  \n",
       "74  „Sollte eine Bestimmung dieses Vertrages unwir...  \n",
       "75  „Keine der Parteien haftet für die Nichterfüll...  \n",
       "\n",
       "[76 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "catalogue_raw = pd.read_excel(\"../data/catalogue_raw.xlsx\")\n",
    "display(catalogue_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01a725ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_topic</th>\n",
       "      <th>section_topic</th>\n",
       "      <th>example</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Projektkosten_und_Zahlungsmodalitäten</td>\n",
       "      <td>Sind sämtliche Kostenarten und -bestandteile (...</td>\n",
       "      <td>im festpreis von € sind sämtliche leistungen e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Projektkosten_und_Zahlungsmodalitäten</td>\n",
       "      <td>Ist das Vergütungsmodell eindeutig festgelegt ...</td>\n",
       "      <td>der kunde zahlt eine monatliche pauschale von ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Projektkosten_und_Zahlungsmodalitäten</td>\n",
       "      <td>Ist ein Zahlungsplan mit konkreten Fälligkeite...</td>\n",
       "      <td>die vergütung ist in drei raten zahlbar bei pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Projektkosten_und_Zahlungsmodalitäten</td>\n",
       "      <td>Sind Währung, Rechnungsstellung, Zahlungsfrist...</td>\n",
       "      <td>alle preise verstehen sich in euro zuzüglich g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Projektkosten_und_Zahlungsmodalitäten</td>\n",
       "      <td>Regelt der Vertrag den Umgang mit Nebenkosten ...</td>\n",
       "      <td>reise und übernachtungskosten werden nur ersta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         paragraph_topic  \\\n",
       "0  Projektkosten_und_Zahlungsmodalitäten   \n",
       "1  Projektkosten_und_Zahlungsmodalitäten   \n",
       "2  Projektkosten_und_Zahlungsmodalitäten   \n",
       "3  Projektkosten_und_Zahlungsmodalitäten   \n",
       "4  Projektkosten_und_Zahlungsmodalitäten   \n",
       "\n",
       "                                       section_topic  \\\n",
       "0  Sind sämtliche Kostenarten und -bestandteile (...   \n",
       "1  Ist das Vergütungsmodell eindeutig festgelegt ...   \n",
       "2  Ist ein Zahlungsplan mit konkreten Fälligkeite...   \n",
       "3  Sind Währung, Rechnungsstellung, Zahlungsfrist...   \n",
       "4  Regelt der Vertrag den Umgang mit Nebenkosten ...   \n",
       "\n",
       "                                             example  \n",
       "0  im festpreis von € sind sämtliche leistungen e...  \n",
       "1  der kunde zahlt eine monatliche pauschale von ...  \n",
       "2  die vergütung ist in drei raten zahlbar bei pr...  \n",
       "3  alle preise verstehen sich in euro zuzüglich g...  \n",
       "4  reise und übernachtungskosten werden nur ersta...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "catalogue_raw[\"example\"] = catalogue_raw[\"example\"].str.strip('„“\"').apply(clean_sections_and_paragraphs)\n",
    "catalogue_raw[\"paragraph_topic\"] = catalogue_raw[\"paragraph_topic\"].apply(\n",
    "    lambda x: x.replace(\"&\", 'und').strip().replace(\" \",\"_\")\n",
    ")\n",
    "\n",
    "catalogue_clean= catalogue_raw.copy()\n",
    "\n",
    "display(catalogue_clean.head())\n",
    "\n",
    "file_path = '../data/catalogue_clean.pkl'  \n",
    "catalogue_clean.to_pickle(file_path)\n",
    "catalogue_clean.to_excel(\"../data/catalogue_clean.xlsx\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e5e14",
   "metadata": {},
   "source": [
    "## 3.2 Embedings \n",
    "### Reusable Embedding Function for Multiple Models\n",
    "Since we plan to experiment with various models throughout the project, and each model generates different embeddings, we created a reusable function that adds a new column to a given dataset. This column contains the embeddings computed by the specified model, based on a target text column. This setup allows us to easily switch between models and store their outputs for further analysis or comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd424425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_embed_text_column(df, text_column, model, target_column, batch_size=16):\n",
    "#     \"\"\"\n",
    "#     Computes SentenceTransformer embeddings column-wise in batches, optimized for CPU performance.\n",
    "#     \"\"\"\n",
    "#     texts = df[text_column].fillna(\"\").tolist()\n",
    "#     all_embeddings = []\n",
    "\n",
    "#     for i in tqdm(range(0, len(texts), batch_size), desc=f\"Embedding {text_column}\"):\n",
    "#         batch = texts[i:i+batch_size]\n",
    "#         with torch.no_grad():\n",
    "#             emb = model.encode(batch, convert_to_tensor=True)\n",
    "#         all_embeddings.extend(emb.cpu().numpy())\n",
    "\n",
    "#     df[target_column] = all_embeddings\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21e39e",
   "metadata": {},
   "source": [
    "## 3.3 Initial Labeling via Embedding Similarity (Deprecated Due to Data Leakage)\n",
    "Our initial approach was to label the dataset using cosine similarity between contract embeddings and requirement catalog examples, based on a selected model and only doublecheked by a human. The idea was to use these labels later to evaluate and compare different classification models.\n",
    "\n",
    "However, as the project progressed, we realized that this method introduced data leakage, since the same embeddings used for labeling were also used during model evaluation. To avoid biased results, we decided to abandon this approach in favor of a more robust labeling strategy.\n",
    "\n",
    "Nevertheless, for the sake of completeness, the original method is documented below.\n",
    "\n",
    "**<=== load data_clean**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcae5793",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m model = SentenceTransformer(modules=[model, pooling_model])\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Embed contract sections\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m train_data_labeled = \u001b[43madd_embed_text_column\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_data_labeled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext_column\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclean_section_content\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_column\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msection_em_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     32\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(catalogue_clean)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Embed catalog examples\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/BIPM/05_NLP/NLP_contract_checker/NLP_Contract_Checker/functions/functions_embeddings.py:8\u001b[39m, in \u001b[36madd_embed_text_column\u001b[39m\u001b[34m(df, text_column, model, target_column, batch_size)\u001b[39m\n\u001b[32m      5\u001b[39m texts = df[text_column].fillna(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m).tolist()\n\u001b[32m      6\u001b[39m all_embeddings = []\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(\u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(texts), batch_size), desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEmbedding \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m      9\u001b[39m     batch = texts[i:i+batch_size]\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[31mNameError\u001b[39m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "path = '../data/data_clean.pkl'\n",
    "train_data_unlabeled = pd.read_pickle(path)\n",
    "\n",
    "# Set seed and sample data\n",
    "random.seed(2211)\n",
    "sample_indices = random.sample(range(len(train_data_unlabeled[\"clean_section_content\"])), k=600)\n",
    "\n",
    "# Choose the model to evaluate\n",
    "model = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "model_name = \"multilingual_paraphrase_sentence\"\n",
    "\n",
    "# Copy and sample data\n",
    "train_data_labeled = train_data_unlabeled.iloc[sample_indices, :].copy()\n",
    "\n",
    "model = models.Transformer(model, max_seq_length=512)\n",
    "\n",
    "pooling_model = models.Pooling(\n",
    "            model.get_word_embedding_dimension(),\n",
    "            pooling_mode_cls_token=False,           \n",
    "            pooling_mode_mean_tokens=True,         # ✅ Mean laut chat gpt besser für semantische übereinstimmung\n",
    "            pooling_mode_max_tokens=False,\n",
    "        )\n",
    "\n",
    "model = SentenceTransformer(modules=[model, pooling_model])\n",
    "\n",
    "# Embed contract sections\n",
    "train_data_labeled = add_embed_text_column(\n",
    "    train_data_labeled,\n",
    "    text_column=\"clean_section_content\",\n",
    "    model= model,\n",
    "    target_column=f\"section_em_{model_name}\"\n",
    ")\n",
    "print(catalogue_clean)\n",
    "\n",
    "# Embed catalog examples\n",
    "catalogue = add_embed_text_column(\n",
    "    catalogue_clean,\n",
    "    text_column=\"example\",\n",
    "    model=model,\n",
    "    target_column=\"emb\"\n",
    ")\n",
    "\n",
    "# Keep only relevant columns\n",
    "train_data_labeled = train_data_labeled[[\"contract\", \"paragraph\", \"section\", \"clean_section_content\", f\"section_em_{model_name}\"]]\n",
    "\n",
    "# Compute cosine similarity\n",
    "X = np.vstack(train_data_labeled[f\"section_em_{model_name}\"].values)\n",
    "Y = np.vstack(catalogue[\"emb\"].values)\n",
    "similarity_matrix = cosine_similarity(X, Y)\n",
    "similarity_percent = np.round(similarity_matrix * 100, 2)\n",
    "\n",
    "# Best matches per section\n",
    "best_match_idx = similarity_matrix.argmax(axis=1)\n",
    "best_match_score = similarity_percent[np.arange(len(X)), best_match_idx]\n",
    "\n",
    "# Store results\n",
    "train_data_labeled[f\"matched_example_index_{model_name}\"] = best_match_idx\n",
    "train_data_labeled[f\"similarity_percent_{model_name}\"] = best_match_score\n",
    "train_data_labeled[f\"matched_example_text_{model_name}\"] = catalogue.loc[best_match_idx, \"example\"].values\n",
    "train_data_labeled[f\"matched_example_topic_{model_name}\"] = catalogue.loc[best_match_idx, \"section_topic\"].values\n",
    "train_data_labeled[f\"matched_paragraph_{model_name}\"] = catalogue.loc[best_match_idx, \"paragraph_topic\"].values\n",
    "\n",
    "# Summary stats\n",
    "print(f\"# {model_name}\")\n",
    "print(\"    mean similarity:\", train_data_labeled[f\"similarity_percent_{model_name}\"].mean())\n",
    "print(\"    max similarity:\", train_data_labeled[f\"similarity_percent_{model_name}\"].max())\n",
    "print(\"    min similarity:\", train_data_labeled[f\"similarity_percent_{model_name}\"].min())\n",
    "\n",
    "# Example: best and worst match\n",
    "train_data_labeled_sorted = train_data_labeled.sort_values(f\"similarity_percent_{model_name}\", ascending=False)\n",
    "best_match = train_data_labeled_sorted.iloc[0]\n",
    "print(\"\\nBest match:\")\n",
    "print(f'{best_match[\"clean_section_content\"]} \\n### to ### \\n{best_match[f\"matched_example_text_{model_name}\"]} \\n### with score ### {best_match[f\"similarity_percent_{model_name}\"]}')\n",
    "\n",
    "train_data_labeled_sorted = train_data_labeled.sort_values(f\"similarity_percent_{model_name}\", ascending=True)\n",
    "worst_match = train_data_labeled_sorted.iloc[0]\n",
    "print(\"\\nWorst match:\")\n",
    "print(f'{worst_match[\"clean_section_content\"]} \\n### to ### \\n{worst_match[f\"matched_example_text_{model_name}\"]} \\n### with score ### {worst_match[f\"similarity_percent_{model_name}\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d7f2ec",
   "metadata": {},
   "source": [
    "This results in a dataset containing the best matches between contract sections and catalog examples for mulitlingual paraphrase models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b595cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(train_data_labeled_sorted.sort_values(f\"similarity_percent_{model_name}\", ascending=False).head(5))\n",
    "\n",
    "subset = train_data_labeled_sorted[['clean_section_content',\"similarity_percent_multilingual_paraphrase_sentence\", 'matched_example_text_multilingual_paraphrase_sentence']].sort_values(\"similarity_percent_multilingual_paraphrase_sentence\", ascending=False)\n",
    "\n",
    "def wrap_text(df, columns):\n",
    "    return df.style.set_properties(**{\n",
    "        'white-space': 'pre-wrap',\n",
    "        'word-wrap': 'break-word'\n",
    "    }, subset=columns)\n",
    "\n",
    "# Example: wrap long text in 2 columns\n",
    "wrap_text(subset.head(5), ['clean_section_content', 'matched_example_text_multilingual_paraphrase_sentence'])\n",
    "\n",
    "len(catalogue)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258772cf",
   "metadata": {},
   "source": [
    "## 3.4 Manual mapping\n",
    "\n",
    "However, as previously mentioned, the automatic labeling approach introduced data leakage. Therefore, we changed our strategy and switched to manual labeling of as many data points as possible.\n",
    "\n",
    "Since this process is extremely time-consuming, and we are not legal experts — even in our native language, German — we were only able to manually label a total of 64 data samples.\n",
    "\n",
    "Another challenge we encountered was that, although we had compiled a dataset of over 1,300 contract sections, we were unable to find suitable matches for all examples in the requirements catalog. As a result, 12 out of the 76 requirement items remain without any corresponding training data.\n",
    "\n",
    "In essence, we manually reviewed all available contracts and initially searched for relevant keywords to identify potential matches. Once potential sections were found, we compared them directly to the example sections in the requirement catalog.\n",
    "\n",
    "Only when we were 100% confident that a contract section semantically matched a catalog entry did we assign the corresponding catalog ID (i.e., the position of the example section within the catalog) to that contract section.\n",
    "\n",
    "The following table shows a selection of sections from our dataset that have been mapped to their corresponding catalog_id (i.e., the index of the requirements catalog)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8573c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlabeled = pd.read_pickle(\"../data/data_clean.pkl\")\n",
    "df_unlabeled = df_unlabeled[[\"contract\",\"paragraph\",\"section\",\"section_content\",\"clean_section_content\"]]\n",
    "display(df_unlabeled)\n",
    "\n",
    "\n",
    "mapping = pd.read_excel(\"../data/mapping_human.xlsx\")\n",
    "print(mapping.head())\n",
    "mapping = mapping[[\"contract\",\"paragraph\", \"section\",\"section_content\",\"catalog_id\"]]\n",
    "display(mapping)\n",
    "\n",
    "# Merge on both 'paragraph' and 'section' for more precise matching\n",
    "df_labeled = mapping.merge(df_unlabeled, how=\"left\", on=[\"contract\",\"paragraph\", \"section\"])\n",
    "df_labeled= df_labeled[[\"contract\",\"paragraph\",\"section\",\"clean_section_content\",\"catalog_id\"]]\n",
    "display(df_labeled)\n",
    "\n",
    "catalogue = pd.read_pickle(\"../data/catalogue_clean.pkl\")\n",
    "catalogue[\"catalog_id\"] = range(1, len(catalogue) + 1)\n",
    "display(catalogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60036c7",
   "metadata": {},
   "source": [
    "# 4. Model Comparrison & Evaluation\n",
    "We now move on to selecting the most suitable model. In total, we chose five different models from Hugging Face, all of which are Sentence Transformers, each with a corresponding tokenizer:\n",
    "\n",
    "- \"deepset/gbert-base\"\n",
    "- \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "- \"bert-base-uncased\",\n",
    "- \"jinaai/jina-embeddings-v2-small-en\",\n",
    "- \"jinaai/jina-embeddings-v2-base-de\",\n",
    "\n",
    "\n",
    "Additionally, we need to decide on a pooling strategy. This strategy determines how we aggregate token-level embeddings into a single section-level embedding, which is essential for comparing sections rather than individual words.\n",
    "When generating embeddings for full sections using transformer-based models, we must reduce the token-level output to a single fixed-size vector. This process is called pooling. Two common strategies are:\n",
    "\n",
    "**CLS Token Pooling**\n",
    "Uses the embedding of the special [CLS] token, which is prepended to every input by models like BERT.\n",
    "The [CLS] token is explicitly trained (during pretraining) to capture the meaning of the entire sentence for classification tasks.\n",
    "Advantage: In models trained for classification (e.g. BERT), the CLS token often encodes global sentence-level semantics efficiently.\n",
    "\n",
    "**Mean Pooling**\n",
    "Averages the embeddings of all tokens (excluding padding).\n",
    "Provides a more balanced representation of the entire sentence or section.\n",
    "Advantage: Especially useful in models not fine-tuned with CLS-specific objectives (e.g. multilingual or paraphrase models), where the semantic information is more evenly distributed across all tokens.\n",
    "\n",
    "**Max Pooling**\n",
    "Takes the maximum value across all token embeddings for each embedding dimension (excluding padding).\n",
    "This emphasizes the most salient features that appear anywhere in the input, regardless of position.\n",
    "\n",
    "Advantage: Max pooling can highlight strong semantic signals (e.g. key terms) by preserving their peak activations. This may be beneficial in tasks where individual high-impact words (rather than holistic meaning) are important — especially in longer or noisy texts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7721adc",
   "metadata": {},
   "source": [
    "## 4.1 Tokenizers and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3d1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "model_names = [\n",
    "    \"gbert-base\",\n",
    "    \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"jina-embeddings-v2-small-en\",\n",
    "    \"jina-embeddings-v2-base-de\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "model_urls = [\n",
    "    \"deepset/gbert-base\",\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"jinaai/jina-embeddings-v2-small-en\",\n",
    "    \"jinaai/jina-embeddings-v2-base-de\"\n",
    "]\n",
    "\n",
    "\n",
    "max_tokens = []\n",
    "\n",
    "for model_name, model_url in zip(model_names, model_urls):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        model = AutoModel.from_pretrained(model_url)\n",
    "    \n",
    "    max_len = model.embeddings.position_embeddings.weight.shape[0]\n",
    "    max_tokens.append((model_name, max_len))\n",
    "print(\"##### ##### #### #### #### \")\n",
    "print(\" \")\n",
    "for model_name, max_token in zip(model_names, max_tokens):\n",
    "    print(f'{model_name} --> max tokens = {max_token}')\n",
    "\n",
    "#### Hier noch beschreieben warum wir diese 5 getestet haben\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf1496c",
   "metadata": {},
   "source": [
    "The following diagram shows how many tokens the various language models generate for the texts in our dataset. It is clearly noticeable that English-based models tend to produce a higher token density, especially for longer texts. A likely reason for this is that German words are not well represented in the English vocabulary, which leads to more frequent splitting into subword tokens.\n",
    "\n",
    "The two models bert-base-uncase and jina-embeddings-v2-small-en generate much more tokens per text segment compared to the German-language models such as gbert-base and jina-embeddings-v2-base-de.\n",
    "\n",
    "Nevertheless, all models, including the English ones, achieve consistently solid performance. The fact that many texts exceed the maximum token input size of the respective models (e.g., 512 for BERT) does not pose a problem for our use case, as we handle this through appropriate truncation or segmentation strategies.\n",
    "\n",
    "Overall, the results indicate that German-language models are better suited for processing German texts, as they require fewer splits and better capture the language’s structure. For our dataset of over 1,300 section texts, language-specific models such as gbert-base, jina-base-de or paraphrase-multilingual-MiniLM-L12-v2 (mulitlingual) prove to be particularly effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87e8497",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Deine Spalte zum Analysieren\n",
    "column = \"clean_section_content\"\n",
    "\n",
    "# Modellnamen und URLs\n",
    "model_names = [\n",
    "    \"gbert-base\",\n",
    "    \"paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"jina-embeddings-v2-small-en\",\n",
    "    \"jina-embeddings-v2-base-de\"\n",
    "]\n",
    "\n",
    "model_urls = [\n",
    "    \"deepset/gbert-base\",\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"jinaai/jina-embeddings-v2-small-en\",\n",
    "    \"jinaai/jina-embeddings-v2-base-de\"\n",
    "]\n",
    "\n",
    "# Tokenizer vorbereiten\n",
    "tokenizers = {}\n",
    "for name, url in zip(model_names, model_urls):\n",
    "    tokenizers[name] = AutoTokenizer.from_pretrained(url)\n",
    "\n",
    "# Tokenlängen erfassen\n",
    "token_counts = {}\n",
    "for name, tokenizer in tokenizers.items():\n",
    "    token_counts[name] = df_structured[column].fillna(\"\").apply(lambda x: len(tokenizer.tokenize(x)))\n",
    "print(token_counts)\n",
    "# Plot: Verteilung der Tokenanzahl (99%-Quantil)\n",
    "plt.figure(figsize=(10, 6))\n",
    "max_x = max(token_counts[name].quantile(0.99) for name in token_counts)\n",
    "bins = np.linspace(0, max_x, 40)\n",
    "\n",
    "styles = ['-', '--', '-.', ':', (0, (3, 1, 1, 1))]\n",
    "\n",
    "for (name, data), style in zip(token_counts.items(), styles):\n",
    "    hist, bin_edges = np.histogram(data, bins=bins, density=True)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    plt.plot(bin_centers, hist, label=name, linestyle=style)\n",
    "\n",
    "plt.title(f\"Tokencount-distribution for: {column}\")\n",
    "plt.xlabel(\"token count\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48446dc",
   "metadata": {},
   "source": [
    "## 4.2 Model Assembly and Pooling start\n",
    "In this section, we load a selection of transformer-based language models and prepare them for use with the Sentence Transformers framework. Each model is wrapped with a pooling strategy to generate fixed-size sentence embeddings.\n",
    "Each model is assembled using the Sentence Transformers Transformer + Pooling modules. We store each configuration in a dictionary (models_dict) using a key that includes the model name and pooling strategy (e.g., base_gbert_sentence_cls).\n",
    "If the RAM is not sufficient there is also the Option to safe all the build models for reuse without reloading from Hugging Face however when doing so they have to be excluded from the git repository since they are to big.\n",
    "This setup provides a flexible and consistent framework to benchmark **15 different transformer** models and pooling strategies for downstream semantic matching tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23db7b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "import os\n",
    "\n",
    "models_in = [\n",
    "    \"deepset/gbert-base\",\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    \"bert-base-uncased\",\n",
    "    \"jinaai/jina-embeddings-v2-small-en\",\n",
    "    \"jinaai/jina-embeddings-v2-base-de\",\n",
    "]\n",
    "\n",
    "models_out = [\n",
    "    \"base_gbert_sentence\",\n",
    "    \"multilingual_paraphrase_sentence\",\n",
    "    \"bert_base_uncased_sentence\",\n",
    "    \"jina_small_en_sentence\",\n",
    "    \"jina_base_de_sentence\",\n",
    "]\n",
    "\n",
    "pool_strats = [\"cls\", \"mean\", \"max\"]\n",
    "\n",
    "# Dictionary zur Sammlung der Modelle\n",
    "models_dict = {}\n",
    "models_urls = {}\n",
    "models_strat = {}\n",
    "\n",
    "\n",
    "for model_in, model_out in zip(models_in, models_out):\n",
    "    word_embedding_model = models.Transformer(model_in, max_seq_length=512)\n",
    "\n",
    "    for pool_strat in pool_strats:\n",
    "        print(f\"Lade Modell: {model_out}, Strategie: {pool_strat}\")\n",
    "\n",
    "        cls = pool_strat == \"cls\"\n",
    "        mean = pool_strat == \"mean\"\n",
    "        maxi = pool_strat == \"max\"\n",
    "\n",
    "        pooling_model = models.Pooling(\n",
    "            word_embedding_model.get_word_embedding_dimension(),\n",
    "            pooling_mode_cls_token=cls,\n",
    "            pooling_mode_mean_tokens=mean,\n",
    "            pooling_mode_max_tokens=maxi,\n",
    "        )\n",
    "\n",
    "        model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "        # Key z. B. \"base_gbert_sentence_cls\"\n",
    "        dict_key = f\"{model_out}_{pool_strat}\"\n",
    "        models_dict[dict_key] = model\n",
    "        models_urls[dict_key] = model_in\n",
    "        models_strat[dict_key] = pool_strat\n",
    "\n",
    "\n",
    "        # Optional speichern:\n",
    "        # model.save(f\"../models/raw_STM/{dict_key}_emb\")\n",
    "\n",
    "# Ausgabe der geladenen Modelle\n",
    "print(\"Geladene Modelle:\", list(models_dict.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3200645",
   "metadata": {},
   "source": [
    "## 4.3 Matching Contract Sections to Catalog Entries Using Embeddings\n",
    "In this section, we match contract sections to the most relevant catalog entries using vector-based semantic similarity. For each embedding model in our comparison, we perform the following steps:\n",
    "\n",
    "Embedding Generation:\n",
    "We encode the contract sections (clean_section_content) and the catalog examples (example) using the selected embedding model.\n",
    "\n",
    "Similarity Computation:\n",
    "We calculate the cosine similarity between each section embedding and all catalog entry embeddings. For each section, we select the catalog entry with the highest similarity score as the predicted match.\n",
    "\n",
    "Ground Truth Comparison:\n",
    "We compare the predicted catalog ID against the ground truth (true_catalog_id) to assess whether the top match is correct.\n",
    "\n",
    "Evaluation with ROC Curve:\n",
    "Using the cosine similarity scores as prediction confidence, we compute an ROC curve and the AUC (Area Under Curve) to measure how well the model distinguishes correct from incorrect matches.\n",
    "\n",
    "Threshold Optimization:\n",
    "We determine an optimal similarity threshold based on the ROC curve (max(TPR - 0.5 × FPR)), which we then use to classify matches as valid or invalid.\n",
    "\n",
    "Postprocessing:\n",
    "Matches below the threshold are marked as invalid by assigning a dummy catalog ID (-99), enabling further analysis and filtering.\n",
    "\n",
    "This analysis is repeated for each model in our benchmark set. The results help us compare model performance and select the best embedding model for semantic contract section matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a9d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "df_to_match = df_labeled[[\"contract\",\"paragraph\",\"section\",\"clean_section_content\"]]\n",
    "df_true_match = df_labeled[[\"contract\",\"paragraph\",\"section\",\"clean_section_content\",\"catalog_id\"]]\n",
    "df_true_match.rename(columns={\"catalog_id\": \"true_catalog_id\"},inplace= True)\n",
    "display(df_true_match)\n",
    "df_true_match[\"true_catalog_id\"] = df_true_match[\"true_catalog_id\"].astype(int)\n",
    "cols = [\"contract\",\"paragraph\",\"section\",\"clean_section_content\"]\n",
    "\n",
    "models_thresholds = {}\n",
    "tab_contents = []\n",
    "print(catalogue)\n",
    "\n",
    "for model_name in models_dict:\n",
    "    model_selected = models_dict[model_name]\n",
    "    df_to_match = add_embed_text_column(df_to_match, text_column=\"clean_section_content\",model = model_selected, target_column=f\"section_em_{model_name}\" )\n",
    "    cat = add_embed_text_column(catalogue, text_column = \"example\", model = model_selected, target_column = \"emb\")\n",
    "    cols.append(f\"section_em_{model_name}\")\n",
    "    df_to_match  = df_to_match[cols]\n",
    "    X = np.vstack(df_to_match[f\"section_em_{model_name}\" ].values)  # Shape: [1400, 768]\n",
    "    Y = np.vstack(cat[\"emb\"].values)                  # Shape: [100, 768]\n",
    "\n",
    "    # Cosine Similarity: alle Kombinationen\n",
    "    similarity_matrix = cosine_similarity(X, Y)  # Shape: [1400, 100]\n",
    "    similarity_percent = np.round(similarity_matrix * 100, 2)  # Skaliert zu 0–100 %\n",
    "\n",
    "    best_match_idx = similarity_matrix.argmax(axis=1)\n",
    "    best_match_score = similarity_percent[np.arange(len(X)), best_match_idx]\n",
    "\n",
    "    # Ergebnisse anhängen\n",
    "    df_to_match[f\"matched_example_index_{model_name}\"] = best_match_idx\n",
    "    df_to_match[f\"similarity_percent_{model_name}\"] = best_match_score\n",
    "    df_to_match[f\"matched_example_text_{model_name}\"] = cat.loc[best_match_idx, \"example\"].values\n",
    "    df_to_match[f\"matched_example_topic_{model_name}\"] = cat.loc[best_match_idx, \"section_topic\"].values\n",
    "    df_to_match[f\"matched_paragraph_{model_name}\"] = cat.loc[best_match_idx, \"paragraph_topic\"].values\n",
    "    df_to_match[f\"matched_catalog_id_{model_name}\"] = cat.loc[best_match_idx, \"catalog_id\"].values\n",
    "\n",
    "    # # print(df_true_match[\"true_catalog_id\"].dtype)\n",
    "    # print(df_to_match[f\"matched_catalog_id_{model_name}\"].dtype)    \n",
    "    \n",
    "\n",
    "    y_true = (df_true_match[\"true_catalog_id\"].values == df_to_match[f\"matched_catalog_id_{model_name}\"].values).astype(int)\n",
    "    y_scores = df_to_match[f\"similarity_percent_{model_name}\"].values / 100  # zurück zu 0–1\n",
    "    # print(\"Verteilung der Klassen in y_true:\")\n",
    "    # print(np.unique(y_true, return_counts=True))\n",
    "\n",
    "\n",
    "    # ROC-Kurve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    custom_score = tpr - 0.5 * fpr\n",
    "    optimal_idx = np.argmax(custom_score)\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "    \n",
    "    output = widgets.Output()\n",
    "    with output:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')  # Diagonale\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curve - {model_name}')\n",
    "        plt.plot(fpr[optimal_idx], tpr[optimal_idx], 'ro', label='Optimal Threshold')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        tab_contents.append((title, output))\n",
    "    # Optimaler Threshold = max(tpr - fpr)\n",
    "\n",
    "    # print(f\"Optimaler Threshold für Cosine Similarity (%): {optimal_threshold * 100:.2f}\")\n",
    "    # Neue Spalte: Match nur wenn Score >= Threshold\n",
    "    df_to_match[f\"match_valid_{model_name}\"] = y_scores >= optimal_threshold\n",
    "    df_to_match.loc[~df_to_match[f\"match_valid_{model_name}\"], f\"matched_catalog_id_{model_name}\"] = -99\n",
    "    cols = list(df_to_match.columns)\n",
    "    # display(df_to_match)\n",
    "    models_thresholds[model_name] = optimal_threshold\n",
    "\n",
    "tab_widget = widgets.Tab()\n",
    "tab_widget.children = [out for _, out in tab_contents]\n",
    "for idx, (name, _) in enumerate(tab_contents):\n",
    "    tab_widget.set_title(idx, name)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display(tab_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cde7ebd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cde44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_matched = df_to_match\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "\n",
    "results = []\n",
    "\n",
    "for model_name in models_dict:\n",
    "    print(f\"### {model_name} ###\")\n",
    "    df_matched_ids = df_matched[[\"contract\",\"paragraph\",\"section\",\"clean_section_content\",f\"matched_catalog_id_{model_name}\"]]\n",
    "    print(\"Accuracy:\", accuracy_score(df_true_match[\"true_catalog_id\"], df_matched_ids[f\"matched_catalog_id_{model_name}\"]))\n",
    "    print(\"F1 (macro):\", f1_score(df_true_match[\"true_catalog_id\"], df_matched_ids[f\"matched_catalog_id_{model_name}\"], average='macro'))\n",
    "    print(\"F1 (weighted):\", f1_score(df_true_match[\"true_catalog_id\"], df_matched_ids[f\"matched_catalog_id_{model_name}\"], average='weighted'))\n",
    "    print(\"\\nReport:\\n\", classification_report(df_true_match[\"true_catalog_id\"], df_matched_ids[f\"matched_catalog_id_{model_name}\"]))\n",
    "\n",
    "    print(f\"### {model_name} ###\")\n",
    "\n",
    "    results.append({\n",
    "            \"model\": model_name,\n",
    "            \"recall (macro)\": recall_score(df_true_match[\"true_catalog_id\"], df_matched_ids[f\"matched_catalog_id_{model_name}\"], average='macro'),\n",
    "            \"recall (weighted)\": recall_score(df_true_match[\"true_catalog_id\"], df_matched_ids[f\"matched_catalog_id_{model_name}\"], average='weighted'),\n",
    "            \"Accuracy\": accuracy_score(df_true_match[\"true_catalog_id\"], df_matched_ids[f\"matched_catalog_id_{model_name}\"]),\n",
    "            \"F1 (macro)\": f1_score(df_true_match[\"true_catalog_id\"], df_matched_ids[f\"matched_catalog_id_{model_name}\"], average='macro'),\n",
    "            \"F1 (weighted)\": f1_score(df_true_match[\"true_catalog_id\"], df_matched_ids[f\"matched_catalog_id_{model_name}\"], average='weighted')\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a7071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "tab_contents = []\n",
    "\n",
    "for model_name in models_dict:\n",
    "    output = widgets.Output()\n",
    "    with output:\n",
    "        # Daten vorbereiten\n",
    "        df_matched_ids = df_matched[[\"contract\", \"paragraph\", \"section\", \"clean_section_content\", f\"matched_catalog_id_{model_name}\"]]\n",
    "        y_true = df_true_match[\"true_catalog_id\"].astype(str)\n",
    "        y_pred = df_matched_ids[f\"matched_catalog_id_{model_name}\"].astype(str)\n",
    "\n",
    "        # Gemeinsame, sortierte Label-Liste für Achsen\n",
    "        # Gemeinsame Labels sammeln\n",
    "        all_labels_set = set(y_true).union(set(y_pred))\n",
    "\n",
    "        # Nach int sortieren, dann in String zurückwandeln\n",
    "        all_labels = [str(x) for x in sorted(map(int, all_labels_set))]\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=all_labels)\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=False, fmt=\"d\", cmap=\"Blues\",\n",
    "                    xticklabels=all_labels, \n",
    "                    yticklabels=all_labels)\n",
    "        plt.title(f\"Confusion Matrix: {model_name}\")\n",
    "        plt.xlabel(\"Predicted Label\")\n",
    "        plt.ylabel(\"True Label\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    tab_contents.append((model_name, output))\n",
    "\n",
    "# Tabs erzeugen\n",
    "tab_widget = widgets.Tab()\n",
    "tab_widget.children = [out for _, out in tab_contents]\n",
    "for idx, (name, _) in enumerate(tab_contents):\n",
    "    tab_widget.set_title(idx, name)\n",
    "\n",
    "display(tab_widget)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa08b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).sort_values(\"recall (weighted)\", ascending=False)\n",
    "thresholds_df = pd.DataFrame(models_thresholds.items(), columns=[\"model\", \"optimal_threshold\"])\n",
    "models_urls_df = pd.DataFrame(models_urls.items(), columns=[\"model\", \"model_url\"])\n",
    "models_strat_df = pd.DataFrame(models_strat.items(), columns=[\"model\", \"pooling_strategy\"])\n",
    "results_df = results_df.merge(models_urls_df, on=\"model\", how=\"left\").merge(models_strat_df, on=\"model\", how=\"left\").merge(thresholds_df, on=\"model\", how=\"left\")\n",
    "display(results_df)\n",
    "results_df.to_csv(\"../data/best_models.csv, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a8dee1",
   "metadata": {},
   "source": [
    "# 5. Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.read_csv(\"../data/best_models.csv)\n",
    "best_model = results_df[[\"model\",\"model_url\",\"pooling_strategy\",\"optimal_threshold\"]].iloc[0]\n",
    "print(best_model)\n",
    "\n",
    "df_unlabeled = pd.read_pickle(\"../data/data_clean.pkl\")\n",
    "df_unlabeled = df_unlabeled[[\"contract\",\"paragraph\",\"section\",\"section_content\",\"clean_section_content\"]]\n",
    "display(df_unlabeled)\n",
    "\n",
    "\n",
    "mapping = pd.read_excel(\"../data/mapping_human.xlsx\")\n",
    "mapping = mapping[[\"contract\",\"paragraph\",\"section\",\"section_content\",\"catalog_id\"]]\n",
    "display(mapping)\n",
    "\n",
    "df_labeled = mapping.merge(df_unlabeled, how = \"left\", on = [\"contract\",\"paragraph\",\"section\"])\n",
    "df_labeled= df_labeled[[\"contract\",\"paragraph\",\"section\",\"clean_section_content\",\"catalog_id\"]]\n",
    "df_labeled = df_labeled[df_labeled[\"catalog_id\"].notna()]\n",
    "\n",
    "\n",
    "print(df_labeled.head)\n",
    "\n",
    "\n",
    "catalogue = pd.read_pickle(\"../data/catalogue_clean.pkl\")\n",
    "catalogue = catalogue.rename(columns={'example': 'clean_section_content'})\n",
    "catalogue = catalogue[['clean_section_content']]\n",
    "catalogue[\"catalog_id\"] = range(len(catalogue))\n",
    "\n",
    "\n",
    "print(catalogue.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c24736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from sentence_transformers.models import Pooling\n",
    "print(\"Model Summary:\")\n",
    "print(\"#####################################################\")\n",
    "print(\"# # # # # # # # # # # # # # # # # # # # # # # # # # #\")\n",
    "print(\"#####################################################\")\n",
    "print(f'Model: {best_model[\"model\"]}') \n",
    "print(\"------------------------------------------------------------\")\n",
    "print(f' embeddings from: {best_model[\"model_url\"]}: ')\n",
    "print(models_dict[best_model[\"model\"]][0].auto_model)\n",
    "print(\"#####################################################\")\n",
    "print(f' pooling strategy: {best_model[\"pooling_strategy\"]}')\n",
    "print(\"------------------------------------------------------------\")\n",
    "print(models_dict[best_model[\"model\"]][1])  \n",
    "print(inspect.getsource(Pooling.forward))\n",
    "print(\"#####################################################\")\n",
    "print(f' classification threashold: {best_model[\"optimal_threshold\"]}')\n",
    "print(\"------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74b0ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_strat = best_model[\"pooling_strategy\"]\n",
    "word_embedding_model = models_dict[best_model[\"model\"]][0]\n",
    "\n",
    "print(\n",
    "    pool_strat\n",
    ")\n",
    "\n",
    "cls = pool_strat == \"cls\"\n",
    "mean = pool_strat == \"mean\"\n",
    "maxim = pool_strat == \"max\"\n",
    "\n",
    "pooling_model = models.Pooling(\n",
    "    word_embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_cls_token=cls,\n",
    "    pooling_mode_mean_tokens=mean,\n",
    "    pooling_mode_max_tokens=maxim,\n",
    ")\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "print(catalogue[\"clean_section_content\"])\n",
    "catalogue_emb = embed_text_column(catalogue, text_column=\"clean_section_content\", model=model, target_column=\"emb\")\n",
    "print(catalogue[\"emb\"])\n",
    "label_embeddings = torch.tensor(np.stack(catalogue[\"emb\"].to_list())).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccd7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CosineMapper(nn.Module):\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         model_name= best_model[\"model_url\"],\n",
    "#         label_embeddings = label_embeddings,\n",
    "#         pooling= best_model[\"pooling_strategy\"],\n",
    "#         threshold= best_model[\"optimal_threshold\"]\n",
    "#     ):\n",
    "#         super().__init__()\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         config = AutoConfig.from_pretrained(model_name)\n",
    "#         self.bert = AutoModel.from_pretrained(model_name,config = config)  # Verwende 'gelu' als Aktivierungsfunktion\n",
    "#         self.pooling = pooling\n",
    "#         self.threshold = threshold\n",
    "\n",
    "#         self.label_embeddings = nn.Parameter(label_embeddings, requires_grad=False)  # z. B. aus SentenceTransformer\n",
    "#         self.activation = nn.GELU()  # Verwende 'gelu' als Aktivierungsfunktion\n",
    "\n",
    "#         self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "#     def forward(self, texts,return_embedding = False  ):\n",
    "#         if isinstance(texts, str):\n",
    "#             texts = [texts]\n",
    "\n",
    "#         inputs = self.tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "#         outputs = self.bert(**inputs)\n",
    "#         token_embeddings = outputs.last_hidden_state  # (B, T, H)\n",
    "\n",
    "#         # Pooling\n",
    "#         if self.pooling == \"cls\":\n",
    "#             pooled = token_embeddings[:, 0]\n",
    "#         elif self.pooling == \"max\":\n",
    "#             mask = inputs[\"attention_mask\"].unsqueeze(-1).expand(token_embeddings.shape).float()\n",
    "#             token_embeddings[mask == 0] = -1e9\n",
    "#             pooled = torch.max(token_embeddings, dim=1)[0]\n",
    "#         elif self.pooling == \"mean\":\n",
    "#             mask = inputs[\"attention_mask\"].unsqueeze(-1).expand(token_embeddings.shape).float()\n",
    "#             summed = torch.sum(token_embeddings * mask, dim=1)\n",
    "#             counts = mask.sum(dim=1).clamp(min=1e-9)\n",
    "#             pooled = summed / counts\n",
    "#         else:\n",
    "#             raise ValueError(\"Unknown pooling method\")\n",
    "\n",
    "#         pooled = self.dropout(pooled)\n",
    "#         if return_embedding:\n",
    "#             return pooled \n",
    "\n",
    "#         # Cosine similarity zu allen Labels\n",
    "#         normalized_input = nn.functional.normalize(pooled, dim=1)\n",
    "#         normalized_labels = nn.functional.normalize(self.label_embeddings, dim=1)\n",
    "\n",
    "#         cosine_sim = torch.matmul(normalized_input, normalized_labels.T)  # (B, num_labels)\n",
    "#         return cosine_sim\n",
    "\n",
    "#     def predict(self, texts, top_k: int = 1, return_scores: bool = False):\n",
    "#         with torch.no_grad():\n",
    "#             scores = self.forward(texts)\n",
    "\n",
    "#         if return_scores:\n",
    "#             # Gib Top-k Indizes (+1) und Scores zurück\n",
    "#             topk_scores, topk_indices = torch.topk(scores, k=top_k, dim=1)\n",
    "#             results = []\n",
    "#             for indices, values in zip(topk_indices, topk_scores):\n",
    "#                 results.append([(i.item(), round(s.item(), 4)) for i, s in zip(indices, values)])\n",
    "#             return results if len(results) > 1 else results[0]\n",
    "\n",
    "#         else:\n",
    "#             # Gib nur Index (+1) der besten Klasse zurück\n",
    "#             preds = torch.argmax(scores, dim=1)\n",
    "#             result = (preds+1 ).tolist()\n",
    "#             return result[0] if len(result) == 1 else result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d1438",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CosineMapper(\n",
    "    model_name=best_model[\"model_url\"],\n",
    "    label_embeddings=label_embeddings,\n",
    "    pooling=best_model[\"pooling_strategy\"],\n",
    "    threshold=best_model[\"optimal_threshold\"],\n",
    ")\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d43592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Alle Texte und true labels holen\n",
    "texts = df_labeled[\"clean_section_content\"].tolist()\n",
    "texts = [str(t) if not isinstance(t, str) else t for t in texts]\n",
    "\n",
    "true_labels = df_labeled[\"catalog_id\"].tolist()  # oder wie deine Label-Spalte heißt\n",
    "# 2. Vorhersagen holen\n",
    "model.return_scores = False  # wichtig: keine Scores, sondern Predictions\n",
    "pred_labels = model.predict(texts)  # gibt List[int]\n",
    "\n",
    "# 3. Klassifikationsbericht\n",
    "print(classification_report(true_labels, pred_labels, digits=3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c34e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1e212",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nur Zeilen behalten, bei denen \"clean_section_content\" nicht NaN ist\n",
    "filtered_df = df_labeled.dropna(subset=[\"clean_section_content\"])\n",
    "\n",
    "# Labels (int) extrahieren\n",
    "filtered_df[\"label\"] = filtered_df[\"catalog_id\"].astype(int)\n",
    "\n",
    "# Listen erzeugen\n",
    "texts = filtered_df[\"clean_section_content\"].tolist()\n",
    "labels = filtered_df[\"label\"].tolist()\n",
    "\n",
    "\n",
    "# Split in Train/Test\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.3,  random_state=42\n",
    ")\n",
    "\n",
    "test_texts, val_texts, test_labels, val_labels = train_test_split(\n",
    "    val_texts,\n",
    "    val_labels,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# class TextLabelDataset(Dataset):\n",
    "#     def __init__(self, texts, labels):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             \"text\": self.texts[idx],\n",
    "#             \"label\": self.labels[idx]\n",
    "#         }\n",
    "train_dataset = TextLabelDataset(train_texts, train_labels)\n",
    "val_dataset = TextLabelDataset(val_texts, val_labels)\n",
    "test_dataset = TextLabelDataset(test_texts, test_labels)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "for batch in train_loader:\n",
    "    print(batch[\"text\"][0])\n",
    "    print(batch[\"label\"][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b66f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100  #number of epochs i.e. how many times is the whole dataset passed through the architecture\n",
    "patience = 5  # Number of epochs to wait before stopping if no improvement\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "optim = AdamW(model.parameters(), lr=5e-5) \n",
    "model.train()  # Set model to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97dfdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "model.return_scores=True\n",
    "print(model(\"test\"))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        texts = batch[\"text\"]\n",
    "        labels = batch[\"label\"] -1\n",
    "        optim.zero_grad()\n",
    "\n",
    "        logits = model(texts)  # cosine similarity (B, num_labels)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        train_total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    avg_train_loss = train_total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            texts = batch[\"text\"]\n",
    "            labels = batch[\"label\"]\n",
    "\n",
    "            logits = model(texts)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_val_preds.extend(preds.cpu().tolist())\n",
    "            all_val_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(\"✅ Model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⛔ Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "# Nach dem Training: bestes Modell laden\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval() #so sieht der aktuelle output aus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798b40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9718e952",
   "metadata": {},
   "source": [
    "# Ohne TEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21998e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "# Nur Zeilen behalten, bei denen \"clean_section_content\" nicht NaN ist\n",
    "filtered_df = df_labeled.dropna(subset=[\"clean_section_content\"])\n",
    "\n",
    "# Labels (int) extrahieren\n",
    "filtered_df[\"label\"] = filtered_df[\"catalog_id\"].astype(int)\n",
    "\n",
    "# Listen erzeugen\n",
    "texts = filtered_df[\"clean_section_content\"].tolist()\n",
    "labels = filtered_df[\"label\"].tolist()\n",
    "\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    texts, labels, test_size=0.2,  random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# class TextLabelDataset(Dataset):\n",
    "#     def __init__(self, texts, labels):\n",
    "#         self.texts = texts\n",
    "#         self.labels = labels\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.texts)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return {\n",
    "#             \"text\": self.texts[idx],\n",
    "#             \"label\": self.labels[idx]\n",
    "#         }\n",
    "train_dataset = TextLabelDataset(train_texts, train_labels)\n",
    "val_dataset = TextLabelDataset(val_texts, val_labels)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c710d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000  #number of epochs i.e. how many times is the whole dataset passed through the architecture\n",
    "patience = 10  # Number of epochs to wait before stopping if no improvement\n",
    "best_val_loss = float(\"inf\")\n",
    "patience_counter = 0\n",
    "model = CosineMapper(\n",
    "    model_name=best_model[\"model_url\"],\n",
    "    label_embeddings=label_embeddings,      \n",
    "    pooling=best_model[\"pooling_strategy\"],\n",
    "    threshold=best_model[\"optimal_threshold\"]\n",
    ")\n",
    "optim = AdamW(model.parameters(), lr=100e-6)    \n",
    "\n",
    "model.train()  # Set model to training mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488b7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of label_embeddings:\", label_embeddings.shape) \n",
    "train_losses = []\n",
    "val_losses = []\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "best_val_loss = float(\"inf\")\n",
    "patience = 10\n",
    "patience_counter = 0\n",
    "model.return_scores=True\n",
    "print(model(\"test\"))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        texts = batch[\"text\"]\n",
    "        labels = batch[\"label\"]-1\n",
    "\n",
    "        optim.zero_grad()\n",
    "\n",
    "        logits = model(texts)  # cosine similarity (B, num_labels)\n",
    "        loss = criterion(logits, labels)\n",
    "\n",
    "        train_total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "    avg_train_loss = train_total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    all_val_preds = []\n",
    "    all_val_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            texts = batch[\"text\"]\n",
    "            labels = batch[\"label\"]\n",
    "\n",
    "            logits = model(texts)\n",
    "            loss = criterion(logits, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            all_val_preds.extend(preds.cpu().tolist())\n",
    "            all_val_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    print(f\"Epoch {epoch + 1}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"best_model_no_test.pth\")\n",
    "        print(\"✅ Model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(\"⛔ Early stopping triggered.\")\n",
    "            break\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef22b36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nach dem Training: bestes Modell laden\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval() #so sieht der aktuelle output aus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45553c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\", marker=\"o\")\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker=\"o\")\n",
    "\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training and Validation Loss Curve\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a8e2e",
   "metadata": {},
   "source": [
    "# 6. Application + Textual checking of requirements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf888b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CosineMapper(\n",
    "    model_name=best_model[\"model_url\"],\n",
    "    label_embeddings=label_embeddings,      \n",
    "    pooling=best_model[\"pooling_strategy\"],\n",
    "    threshold=best_model[\"optimal_threshold\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df_labeled = df_labeled[pd.notna(df_labeled[\"contract\"])]\n",
    "contracts_with_labels = df_labeled[\"contract\"].astype(int).unique()\n",
    "np.append(contracts_with_labels, 1)\n",
    "\n",
    "print(contracts_with_labels)\n",
    "\n",
    "\n",
    "contracts= pd.read_pickle(\"../data/data_scraped_input_relevant.pkl\")\n",
    "contracts[\"contract\"] = contracts[\"contract\"].astype(int)\n",
    "contracts_without_labels = contracts[~contracts[\"contract\"].isin(contracts_with_labels)][\"contract\"].unique().tolist()\n",
    "exp_contract = 26\n",
    "exp = contracts[contracts[\"contract\"] == exp_contract]  # Beispiel: zufällige 1 Verträge\n",
    "print(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421fe4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# class SectionTopicPredictor:\n",
    "#     def __init__(self, model, catalogue_clean):\n",
    "#         \"\"\"\n",
    "#         Parameters:\n",
    "#         - model: dein CosineMapper mit predict()-Methode\n",
    "#         - catalogue_clean: DataFrame mit Spalte 'section_topic'\n",
    "#         \"\"\"\n",
    "#         self.model = model\n",
    "#         self.catalogue_clean = catalogue_clean\n",
    "\n",
    "#     def _preprocess_contract(self, text):\n",
    "#         \"\"\"\n",
    "#         Führt Paragraph- und Abschnittsextraktion + Cleaning durch.\n",
    "#         \"\"\"\n",
    "#         fake_row = {\"content\": text, \"contract\": 1}\n",
    "#         sections = extract_paragraphs_and_sections(fake_row)  # -> List[Dict]\n",
    "#         for section in sections:\n",
    "#             section[\"clean_section_content\"] = clean_sections_and_paragraphs(section[\"section_content\"])\n",
    "#         return sections\n",
    "\n",
    "#     def predict_contract(self, contract_text, return_topic_score=False):\n",
    "#         \"\"\"\n",
    "#         contract_text: Volltext eines Vertrags (String)\n",
    "\n",
    "#         Rückgabe: DataFrame mit Feldern aus den extrahierten Sections + 'predicted_topic'\n",
    "#         \"\"\"\n",
    "#         cont_exp = self._preprocess_contract(contract_text)\n",
    "#         return self.predict_sections(cont_exp, return_topic_score=return_topic_score)\n",
    "\n",
    "#     def predict_sections(self, cont_exp, return_topic_score=False):\n",
    "#         \"\"\"\n",
    "#         cont_exp: Liste von Dicts mit mindestens 'section', 'section_content', 'clean_section_content'\n",
    "\n",
    "#         Rückgabe: DataFrame mit allen ursprünglichen Feldern + 'predicted_topic'\n",
    "#         \"\"\"\n",
    "#         records = []\n",
    "\n",
    "#         for section in cont_exp:\n",
    "#             cleaned = section['clean_section_content']\n",
    "# #\n",
    "#             if return_topic_score:\n",
    "#                 model_output = self.model.predict(cleaned, return_scores=True)\n",
    "\n",
    "#                 top = max(model_output, key=lambda x: x[1])\n",
    "#                 label = top[0]\n",
    "#                 score = top[1]\n",
    "#                 index = int(label) - 1 \n",
    "#             else:\n",
    "#                 label = self.model.predict(cleaned, return_scores=False)\n",
    "#                 index = int(label) - 1\n",
    "#                 score = None\n",
    "\n",
    "#             topic = self.catalogue_clean[\"section_topic\"].iloc[index]\n",
    "\n",
    "#             record = {**section, \"predicted_topic\": topic}\n",
    "#             if return_topic_score:\n",
    "#                 record[\"score\"] = score\n",
    "\n",
    "#             records.append(record)\n",
    "\n",
    "#         return pd.DataFrame(records)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53fe56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "top_scores = []\n",
    "top_sections = []\n",
    "top_indexes = []\n",
    "print(catalogue_clean)\n",
    "print(exp[\"content\"])\n",
    "predictor = SectionTopicPredictor(model, catalogue_clean)\n",
    "\n",
    "for i in contracts_without_labels:\n",
    "\n",
    "    exp = contracts[contracts[\"contract\"] == i] \n",
    "    result_df = predictor.predict_contract(exp[\"content\"].values[0], return_topic_score=True)\n",
    "\n",
    "    # Ausgabe (optional)\n",
    "    print(result_df.sort_values(\"score\", ascending=False).head(1))\n",
    "    sorted_df = result_df.sort_values(\"score\", ascending=False)\n",
    "\n",
    "    top_score = sorted_df.iloc[0][\"score\"]\n",
    "    top_section = sorted_df.iloc[0][\"section\"]\n",
    "    index_topscore = result_df[result_df[\"section\"] == top_section].index[0]  # Index des Top-Scores\n",
    "    top_scores.append(top_score)  \n",
    "    top_sections.append(top_section)  \n",
    "    top_indexes.append(index_topscore)  # Index des Top-Scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df251547",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(contracts_without_labels)\n",
    "print(top_scores)\n",
    "print(top_sections)\n",
    "print(top_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0002328f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = contracts[contracts[\"contract\"] == 16] \n",
    "for i in range(len(exp)):  \n",
    "    text = exp[\"content\"].values[0]\n",
    "    predicted_df = predictor.predict_contract(text, return_topic_score=True)\n",
    "    #print(predicted_df)\n",
    "\n",
    "# Gute Kombi z.b contract 26 section 73\n",
    "random_index = 35 #random.randint(0, len(predicted_df) - 1)\n",
    "\n",
    "topic = predicted_df.loc[random_index, \"predicted_topic\"]\n",
    "content = predicted_df.loc[random_index, \"section_content\"]\n",
    "score = predicted_df.loc[random_index, \"score\"]\n",
    "\n",
    "print(\"🔹 Predicted Topic:\\n\", topic)\n",
    "print(\"\\n🔸 Section Content:\\n\", content)\n",
    "# Print Section Content in green color\n",
    "print(\"\\033[92m🔥 Score:\\033[0m\", score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5272a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functioniert gut Vertrag 26 index 68\n",
    "random_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted_df)\n",
    "results_df = predicted_df.sort_values(\"score\")\n",
    "catalogue = pd.read_excel(\"../data/catalogue_clean_mit_aspects.xlsx\")\n",
    "print(catalogue)\n",
    "\n",
    "print(result_df[[\"predicted_topic\",\"section_content\",\"score\"]].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e495f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1dabf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "client = OpenAI(api_key=OpenAiKey)\n",
    "\n",
    "def check_core_aspects_with_llm(section_text, core_aspects, model=\"gpt-4o-mini\", sleep_between_calls=1.5):\n",
    "    aspects_list = \"\\n- \" + \"\\n- \".join(core_aspects)\n",
    "    prompt = f\"\"\"Du bist ein Vertragsexperte. Prüfe den folgenden Vertragstext auf die Einhaltung der folgenden Kernanforderungen (Core Aspects).\n",
    "\n",
    "Gib als Ergebnis für jeden einzelnen Punkt einen Erfüllungsgrad von 0 bis 1 an (0 = nicht erfüllt, 1 = voll erfüllt, 0.5 = teilweise erfüllt). Gib zusätzlich eine durchschnittliche Erfüllungsquote in Prozent für alle Core Aspects an.\n",
    "\n",
    "Vertragstext:\n",
    "{section_text}\n",
    "\n",
    "Core Aspects:{aspects_list}\n",
    "\n",
    "Antwortformat (nur JSON):\n",
    "{{\n",
    "\"core_aspect_scores\": {{\n",
    "    \"Aspekt 1\": 1,\n",
    "    \"Aspekt 2\": 0.5\n",
    "}},\n",
    "\"average_fulfillment_percent\": 76.5\n",
    "}}\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.2\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        print(\"API-Fehler:\", e)\n",
    "        return None\n",
    "    finally:\n",
    "        time.sleep(sleep_between_calls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf45133",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(predicted_df.columns)\n",
    "print(catalogue.columns)\n",
    "\n",
    "df_eval = predicted_df.merge(\n",
    "    catalogue[[\"section_topic\", \"core_aspects\"]],\n",
    "    left_on=\"predicted_topic\",\n",
    "    right_on=\"section_topic\",\n",
    "    how=\"left\")\n",
    "\n",
    "# df_eval = df_eval[[\"contract\", \"paragraph\", \"section\", \"clean_section_content\", \"predicted_topic\", \"core_aspects\"]]\n",
    "df_eval = df_eval[[\"contract\", \"paragraph\", \"section\", \"section_content\", \"predicted_topic\",\"score\", \"core_aspects\"]]\n",
    "\n",
    "\n",
    "\n",
    "print(df_eval.head() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df6995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "# Bewertungsfunktion\n",
    "def evaluate_llm(row):\n",
    "    section_text = row[\"section_content\"]  # section_text = row[\"clean_section_content\"]\n",
    "    aspects = [line.strip() for line in str(row[\"core_aspects\"]).split(\"\\n\") if line.strip()]\n",
    "    raw_response = check_core_aspects_with_llm(section_text, aspects)\n",
    "    # Versuche, reines JSON aus der Antwort zu extrahieren\n",
    "    try:\n",
    "        # Sonderfall: Antwort enthält ```json ... ``` oder anderen Markdown-Block\n",
    "        match = re.search(r\"{.*}\", raw_response, re.DOTALL)\n",
    "        if match:\n",
    "            cleaned_json = match.group(0)\n",
    "            print(\"✅ LLM-Antwort:\", cleaned_json)\n",
    "            return json.loads(cleaned_json)\n",
    "        else:\n",
    "            raise ValueError(\"Kein JSON-Block gefunden.\")\n",
    "    except Exception:\n",
    "        print(\"❌ Parsing-Fehler. Antwort war:\", raw_response)\n",
    "        return {\"core_aspect_scores\": {}, \"average_fulfillment_percent\": None}\n",
    "\n",
    "\n",
    "# LLM-Auswertung durchführen\n",
    "# Zufällige Auswahl von 10 Zeilen für LLM-Auswertung\n",
    "random_indices = random.sample(range(len(df_eval)), 20)\n",
    "df_eval_subset = df_eval#.iloc[random_indices].copy()\n",
    "\n",
    "df_eval_subset[\"llm_eval_result\"] = df_eval_subset.apply(evaluate_llm, axis=1)\n",
    "df_eval.loc[df_eval_subset.index, \"llm_eval_result\"] = df_eval_subset[\"llm_eval_result\"]\n",
    "\n",
    "df_eval[\"core_aspect_scores\"] = df_eval[\"llm_eval_result\"].apply(\n",
    "    lambda x: x.get(\"core_aspect_scores\", {}) if isinstance(x, dict) else {}\n",
    ")\n",
    "df_eval[\"average_fulfillment_percent\"] = df_eval[\"llm_eval_result\"].apply(\n",
    "    lambda x: x.get(\"average_fulfillment_percent\") if isinstance(x, dict) else None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08bf70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Nur Zeilen anzeigen, bei denen \"average_fulfillment_percent\" nicht None ist\n",
    "sorted = df_eval[df_eval[\"average_fulfillment_percent\"].notnull()].sort_values(\"average_fulfillment_percent\", ascending=False).head()\n",
    "display(sorted)\n",
    "if not sorted.empty:\n",
    "\tfirst_row = sorted.iloc[1]\n",
    "\tprint(f'content: {first_row[\"section_content\"]}')\n",
    "\tprint(f'map to : {first_row[\"predicted_topic\"]}')\n",
    "\tprint(f'core aspects: {first_row[\"core_aspects\"]}')\n",
    "\tprint(f'average fulfillment percent: {first_row[\"average_fulfillment_percent\"]}')   \n",
    "\tprint(f'core aspect scores: {first_row[\"core_aspect_scores\"]}')\n",
    "# Optionally save the evaluated results\n",
    "df_eval.to_pickle(\"../data/llm_eval_result.pkl\")\n",
    "df_eval.to_excel(\"../data/llm_eval_result.xlsx\", index=False)\n",
    "# preview first few results\n",
    "#display(sorted[[\"section_content\", \"core_aspect_scores\", \"average_fulfillment_percent\"]].head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
